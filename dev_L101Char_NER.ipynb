{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"“L101Char_NER.ipynb”的副本","provenance":[{"file_id":"1oZ9LbmWbzTdlHZte6xuZzy4PUSshZvNH","timestamp":1610464350526}]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T9wR6bEM5tI_","executionInfo":{"status":"ok","timestamp":1610473329504,"user_tz":0,"elapsed":9873,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"c76e91e8-d9d1-4de1-e448-4472ff4b42ec"},"source":["# Setup the google colab\n","!pip install xmnlp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting xmnlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/62/bee2665a3f757f0e8b918ba8610e389f6e25392489dbd889b5ed9addc0ca/xmnlp-0.2.3.tar.gz (23.7MB)\n","\u001b[K     |████████████████████████████████| 23.7MB 137kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.6/dist-packages (from xmnlp) (1.19.5)\n","Building wheels for collected packages: xmnlp\n","  Building wheel for xmnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for xmnlp: filename=xmnlp-0.2.3-cp36-none-any.whl size=23707964 sha256=7b25bebf8dd03dcaac6b3a1aad0df0a64b29a97eda7d8e87662c21a191b599ef\n","  Stored in directory: /root/.cache/pip/wheels/1d/05/70/4a9b15884cdd6997fbc006fd6a9c2b3f5ca66857b6ac37100c\n","Successfully built xmnlp\n","Installing collected packages: xmnlp\n","Successfully installed xmnlp-0.2.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lg52UOO05tJJ","executionInfo":{"status":"ok","timestamp":1610473391801,"user_tz":0,"elapsed":72158,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"9d43a272-0acf-4a11-fd42-b9e919bef0df"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zLcVgqZ95tJJ","executionInfo":{"status":"ok","timestamp":1610473395396,"user_tz":0,"elapsed":719,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}}},"source":["isColab = True\n","\n","googlePath = \"./drive/MyDrive/L101Project/\" if isColab else \"./\"\n","cpuPools = 2 if isColab else 8"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsqRMltL5tJK","executionInfo":{"status":"ok","timestamp":1610473399563,"user_tz":0,"elapsed":3434,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","physical_devices = tf.config.list_physical_devices('GPU')\n","# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKjkzqY95tJL","executionInfo":{"status":"ok","timestamp":1610473399564,"user_tz":0,"elapsed":2188,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}}},"source":["# from tensorflow.compat.v1 import ConfigProto\n","# from tensorflow.compat.v1 import Session\n","\n","# config = ConfigProto()\n","# config.gpu_options.allow_growth = True\n","# session = Session(config=config)\n","# tf.compat.v1.disable_eager_execution()\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"AxeFAx7d5tJL","executionInfo":{"status":"ok","timestamp":1610473401329,"user_tz":0,"elapsed":3154,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}}},"source":["import csv\n","import numpy as np\n","import xmnlp\n","import itertools\n","\n","import collections\n","import operator\n","from functools import reduce\n","import json\n","from multiprocessing import Pool\n","from keras.preprocessing.sequence import pad_sequences"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"DGrQh6Rt5tJM","executionInfo":{"status":"ok","timestamp":1610473401330,"user_tz":0,"elapsed":1816,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}}},"source":[""],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"zrav-7lOjXRU","executionInfo":{"status":"ok","timestamp":1610473401332,"user_tz":0,"elapsed":1199,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}}},"source":["# Load NER dataset"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"1AFh1_WI5tJM","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1610474041386,"user_tz":0,"elapsed":736,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"6007f88e-8f92-47cd-8521-4a218d09a7a1"},"source":["import pandas as pd\n","train_ner_path =  \"./ner_train.txt\"\n","train = pd.read_table(train_ner_path, header=None, names=['token', 'label'])  # don't drop the empty lines yet, they show up as NaN in the data frame\n","train.head(n=50)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>海</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>钓</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>比</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>赛</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>地</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>点</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>在</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>厦</td>\n","      <td>B-LOC</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>门</td>\n","      <td>I-LOC</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>与</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>金</td>\n","      <td>B-LOC</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>门</td>\n","      <td>I-LOC</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>之</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>间</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>的</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>海</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>域</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>。</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>这</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>座</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>依</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>山</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>傍</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>水</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>的</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>博</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>物</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>馆</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>由</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>国</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>内</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>一</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>流</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>的</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>设</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>计</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>师</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>主</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>持</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>设</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>计</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>，</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>整</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>个</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>建</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>筑</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>群</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>精</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>美</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   token  label\n","0      海      O\n","1      钓      O\n","2      比      O\n","3      赛      O\n","4      地      O\n","5      点      O\n","6      在      O\n","7      厦  B-LOC\n","8      门  I-LOC\n","9      与      O\n","10     金  B-LOC\n","11     门  I-LOC\n","12     之      O\n","13     间      O\n","14     的      O\n","15     海      O\n","16     域      O\n","17     。      O\n","18   NaN    NaN\n","19     这      O\n","20     座      O\n","21     依      O\n","22     山      O\n","23     傍      O\n","24     水      O\n","25     的      O\n","26     博      O\n","27     物      O\n","28     馆      O\n","29     由      O\n","30     国      O\n","31     内      O\n","32     一      O\n","33     流      O\n","34     的      O\n","35     设      O\n","36     计      O\n","37     师      O\n","38     主      O\n","39     持      O\n","40     设      O\n","41     计      O\n","42     ，      O\n","43     整      O\n","44     个      O\n","45     建      O\n","46     筑      O\n","47     群      O\n","48     精      O\n","49     美      O"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"ECtlM9rs5tJN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610474041388,"user_tz":0,"elapsed":496,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"77ab8d1d-6013-4b1f-8e64-eb9ede9c4bf4"},"source":["set(train[\"label\"])"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O', nan}"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":979},"id":"yNYol2W9uN5g","executionInfo":{"status":"ok","timestamp":1610474090192,"user_tz":0,"elapsed":17148,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"ebcb5539-b35b-4bb2-89d9-6b76794dccbb"},"source":["import numpy as np\n","\n","# in order to convert word tokens to integers: list the set of token types\n","token_vocab = train.token.unique().tolist()\n","oov = len(token_vocab)  # OOV (out of vocabulary) token as vocab length (because that's max.index + 1)\n","\n","# convert word tokens to integers\n","def token_index(tok):\n","  ind = tok\n","  if not pd.isnull(tok):  # new since last time: deal with the empty lines which we didn't drop yet\n","    if tok in token_vocab:  # if token in vocabulary\n","      ind = token_vocab.index(tok)\n","    else:  # else it's OOV\n","      ind = oov\n","  return ind\n","\n","# training labels: convert BIO to integers\n","def bio_index(bio):\n","  ind = bio\n","  if not pd.isnull(bio):  # deal with empty lines\n","    if bio=='B-LOC':\n","      ind = 0\n","    elif bio=='B-ORG':\n","      ind = 1\n","    elif bio=='B-PER':\n","      ind = 2\n","    elif bio=='I-LOC':\n","      ind = 3\n","    elif bio=='I-ORG':\n","      ind = 4\n","    elif bio=='I-PER':\n","      ind = 5\n","    elif bio=='O':\n","      ind = 6\n","  return ind\n","\n","# pass a data frame through our feature extractor\n","def extract_features(txt_orig,istest=False):\n","  txt = txt_orig.copy()\n","  tokinds = [token_index(u) for u in txt['token']]\n","  txt['token_indices'] = tokinds\n","  if not istest:  # can't do this with the test set\n","    bioints = [bio_index(b) for b in txt['label']]\n","    txt['bio'] = bioints\n","  return txt\n","\n","train_copy = extract_features(train)\n","train_copy.head(n=30)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>label</th>\n","      <th>token_indices</th>\n","      <th>bio</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>海</td>\n","      <td>O</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>钓</td>\n","      <td>O</td>\n","      <td>1.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>比</td>\n","      <td>O</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>赛</td>\n","      <td>O</td>\n","      <td>3.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>地</td>\n","      <td>O</td>\n","      <td>4.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>点</td>\n","      <td>O</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>在</td>\n","      <td>O</td>\n","      <td>6.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>厦</td>\n","      <td>B-LOC</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>门</td>\n","      <td>I-LOC</td>\n","      <td>8.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>与</td>\n","      <td>O</td>\n","      <td>9.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>金</td>\n","      <td>B-LOC</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>门</td>\n","      <td>I-LOC</td>\n","      <td>8.0</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>之</td>\n","      <td>O</td>\n","      <td>11.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>间</td>\n","      <td>O</td>\n","      <td>12.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>的</td>\n","      <td>O</td>\n","      <td>13.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>海</td>\n","      <td>O</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>域</td>\n","      <td>O</td>\n","      <td>14.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>。</td>\n","      <td>O</td>\n","      <td>15.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>这</td>\n","      <td>O</td>\n","      <td>17.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>座</td>\n","      <td>O</td>\n","      <td>18.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>依</td>\n","      <td>O</td>\n","      <td>19.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>山</td>\n","      <td>O</td>\n","      <td>20.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>傍</td>\n","      <td>O</td>\n","      <td>21.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>水</td>\n","      <td>O</td>\n","      <td>22.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>的</td>\n","      <td>O</td>\n","      <td>13.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>博</td>\n","      <td>O</td>\n","      <td>23.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>物</td>\n","      <td>O</td>\n","      <td>24.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>馆</td>\n","      <td>O</td>\n","      <td>25.0</td>\n","      <td>6.0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>由</td>\n","      <td>O</td>\n","      <td>26.0</td>\n","      <td>6.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   token  label  token_indices  bio\n","0      海      O            0.0  6.0\n","1      钓      O            1.0  6.0\n","2      比      O            2.0  6.0\n","3      赛      O            3.0  6.0\n","4      地      O            4.0  6.0\n","5      点      O            5.0  6.0\n","6      在      O            6.0  6.0\n","7      厦  B-LOC            7.0  0.0\n","8      门  I-LOC            8.0  3.0\n","9      与      O            9.0  6.0\n","10     金  B-LOC           10.0  0.0\n","11     门  I-LOC            8.0  3.0\n","12     之      O           11.0  6.0\n","13     间      O           12.0  6.0\n","14     的      O           13.0  6.0\n","15     海      O            0.0  6.0\n","16     域      O           14.0  6.0\n","17     。      O           15.0  6.0\n","18   NaN    NaN            NaN  NaN\n","19     这      O           17.0  6.0\n","20     座      O           18.0  6.0\n","21     依      O           19.0  6.0\n","22     山      O           20.0  6.0\n","23     傍      O           21.0  6.0\n","24     水      O           22.0  6.0\n","25     的      O           13.0  6.0\n","26     博      O           23.0  6.0\n","27     物      O           24.0  6.0\n","28     馆      O           25.0  6.0\n","29     由      O           26.0  6.0"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"id":"QeR_hQKmuNt9","executionInfo":{"status":"error","timestamp":1610476808110,"user_tz":0,"elapsed":2626739,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"6dc403c5-a8bd-4a9c-d478-4d868439c891"},"source":["def tokens2sequences(txt_orig,istest=False):\n","  '''\n","  Takes panda dataframe as input, copies, and adds a sequence index based on full-stops.\n","  Outputs a dataframe with sequences of tokens, named entity labels, and token indices as lists.\n","  '''\n","  txt = txt_orig.copy()\n","  txt['sequence_num'] = 0\n","  seqcount = 0\n","  for i in txt.index:  # in each row...\n","    txt.loc[i,'sequence_num'] = seqcount  # set the sequence number\n","    if pd.isnull(txt.loc[i,'token']):  # increment sequence counter at empty lines\n","      seqcount += 1\n","  # now drop the empty lines, group by sequence number and output df of sequence lists\n","  txt = txt.dropna()\n","  if istest:  # test set doesn't have labels\n","    txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'token_indices', \"upos\"]].agg(lambda x: list(x))\n","  else:\n","    txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'bio', 'token_indices', \"label\"]].agg(lambda x: list(x))\n","  return txt_seqs\n","\n","print(\"This cell takes a little while to run: be patient :)\")\n","train_seqs = tokens2sequences(train_copy)\n","train_seqs.head()"],"execution_count":19,"outputs":[{"output_type":"stream","text":["This cell takes a little while to run: be patient :)\n"],"name":"stdout"},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-2c8fb3d19d2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This cell takes a little while to run: be patient :)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens2sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtrain_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-2c8fb3d19d2f>\u001b[0m in \u001b[0;36mtokens2sequences\u001b[0;34m(txt_orig, istest)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtxt_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequence_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'token_indices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"upos\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtxt_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequence_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bio_only'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'token_indices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'upos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtxt_seqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m             )\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mbad_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Columns not found: {str(bad_keys)[1:-1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Columns not found: 'upos', 'bio_only'\""]}]},{"cell_type":"code","metadata":{"id":"4u-6M3StuNhF"},"source":["# Chinese word segamentation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBlBOICM5tJP"},"source":["pkuTrain = googlePath + \"./corpus/cws/icwb2-data/training/pku_training.utf8\"\n","pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n","\n","f = open(pkuTest, \"r\")\n","temp = f.readlines()\n","con = 0\n","for item in temp:\n","    con+=1\n","    if(item == \"\\n\"):\n","        print(con)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ya4Cqu7d5tJP"},"source":["## Create Corpus for Chinese Word Segamentation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCtgVVrD5tJQ"},"source":["import pandas as pd\n","pkuTrain = googlePath + \"./corpus/cws/icwb2-data/training/pku_training.utf8\"\n","pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n","train = pd.read_table(pkuTrain,  encoding='utf8', header=None, names=['input'])  # don't drop the empty lines yet, they show up as NaN in the data frame\n","# train.head(n=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_W4q4RcB5tJQ"},"source":["pkuTest = googlePath  + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n","test = pd.read_table(pkuTest,  encoding='utf8', header=None, names=['input']) \n","test[\"raws\"] = test[\"input\"]\n","# test.head(n=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WgYuVNquRxf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jORKwaO8uRnC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_n8SNsruRcS"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcXFQ8iQuQ5d"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eKq-9B_u5tJR","executionInfo":{"status":"ok","timestamp":1610461470524,"user_tz":0,"elapsed":163339,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"594ac4c4-e8d4-4d60-ed57-c93ef57d254f"},"source":["def prepross_data(train_Data):\n","    inputs = train_Data[\"input\"][:]\n","    print(len(inputs))\n","    raws = []\n","    tokenList = []\n","    \n","    for item in inputs:\n","        item = item.replace(\"   \", \"  \")\n","\n","        raw = item.replace(\" \", \"\")\n","#         print(item)\n","#         print(raw)\n","        raws.append(raw)\n","        tokens = (item.split(\"  \")[:-1])\n","        tokenList.append(tokens)\n","#         print(tokens)\n","    train_Data[\"raws\"] = raws\n","    train_Data[\"tokenList\"] = tokenList\n","    dictionary_train_word = list(set(reduce(operator.add,train_Data[\"tokenList\"])))\n","    dictionary_train_char =  list(set(reduce(operator.add,train_Data[\"raws\"])))    \n","    return train_Data, dictionary_train_char, dictionary_train_word\n","train, train_dic_char, train_dic_word  = prepross_data(train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["19054\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCfwloea5tJT","executionInfo":{"status":"ok","timestamp":1610461470525,"user_tz":0,"elapsed":163235,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"aa4c8b9b-a62a-4192-f937-c6e28b048b20"},"source":["# Get redical dictionary\n","\n","# Get redical of a chinese character:\n","def str_get_redical(st):\n","    return xmnlp.radical(st)\n","\n","def get_redical_dic_from_char_dic(char_dic):\n","    return list(set(str_get_redical(\"\".join(char_dic))))\n","\n","train_dic_redical = get_redical_dic_from_char_dic(train_dic_char)\n","print(len(train_dic_redical))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(Lazy Load) Loading model...\n","235\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xALvoxUR5tJU"},"source":["# Build up subcharacter dictionary\n","# Build up Chinese sub-character unites diactionary.\n","chaiZi_Dic = {}\n","with open(googlePath + \"corpus/chaizi/chaizi-jt.txt\", 'r') as f:\n","    reader = csv.reader(f,delimiter='\\t')\n","    for row in reader:\n","        chaiZi_Dic[row[0]] = row[1]\n","        \n","def getSubChar(charCN):\n","    if(charCN in chaiZi_Dic.keys()):\n","        return chaiZi_Dic[charCN].split(\" \")\n","    elif charCN in train_dic_char:\n","        return [charCN]\n","    else:\n","        return [None]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvz3faWu5tJU","executionInfo":{"status":"ok","timestamp":1610461470529,"user_tz":0,"elapsed":163084,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"deb994bd-a004-46ad-8c88-371b4c333cec"},"source":["def str_get_subchar(st):\n","#     for item in st:\n","#         print(item)\n","#         print(getSubChar(item))\n","    return [getSubChar(item) for item in st]\n","\n","def get_subchar_dic_from_char_dic(char_dic):\n","    return list(set(itertools.chain.from_iterable(str_get_subchar(\"\".join(char_dic)))))\n","\n","train_dic_subchar = get_subchar_dic_from_char_dic(train_dic_char)\n","print(len(train_dic_subchar))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1328\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J_RrfQrx5tJV"},"source":["# Validate the pre-processing\n","def validate_data(data):\n","    tkList = data[\"tokenList\"]\n","    raws = data[\"raws\"]\n","    for (tokens, raw) in zip(tkList,raws):\n","#         print(tokens)\n","        \n","#         print(raw)\n","        temp  = \"\".join(tokens)\n","#         print(temp)\n","        assert(temp == raw)\n","        if not (temp == raw):\n","            print(temp)\n","            print(raw)\n","            print(tokens)\n","validate_data(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HaTqzXQY5tJV"},"source":["# print(len(train_dic_char))\n","# print(len(set(train_dic_char)))\n","# print(len(train_dic_word))\n","# print(len(set(train_dic_word)))\n","# with open(googlePath + \"./corpus/cws/icwb2-data/gold/pku_training_words.utf8\", 'r') as f:\n","#     content = f.readlines()  \n","#     print(len(content))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"iaChv4nZ5tJW","executionInfo":{"status":"ok","timestamp":1610461470533,"user_tz":0,"elapsed":162347,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"5e145fdb-8bf8-4441-fd82-36c62cdc9acc"},"source":["train['raws'][1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'中共中央总书记、国家主席江泽民'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CT30UqI5tJX","executionInfo":{"status":"ok","timestamp":1610461470765,"user_tz":0,"elapsed":162293,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"662bbf11-6de4-4da4-ed19-80629166ed96"},"source":["# Feature extraction\n","\n","oov = len(train_dic_char)\n","print(oov)\n","\n","oov_redical = len(train_dic_redical)\n","print(oov_redical)\n","\n","oov_subchar = len(train_dic_subchar)\n","print(oov_subchar)\n","\n","def token_index(tok):\n","    ind = tok\n","    if tok in train_dic_char:  # if token in vocabulary\n","        ind = train_dic_char.index(tok)\n","    else:  # else it's OOV\n","        ind = oov\n","    return ind\n","\n","def str_token_index(string):\n","    return [token_index(x) for x in (string)]\n","\n","\n","\n","\n","def redical_index(red):\n","    ind = oov_redical\n","    if red in train_dic_redical:  # if token in vocabulary\n","        ind = train_dic_redical.index(red)\n","    return ind\n","\n","def str_red_index(string):\n","    return [redical_index(x) for x in str_get_redical(string)]\n","\n","\n","def subchar_index(subchar):\n","    ind = oov_subchar\n","    if subchar in train_dic_subchar:\n","        ind = train_dic_subchar.index(subchar)\n","    return ind\n","\n","def str_subchar_index(string):\n","    return [[subchar_index(temp) for temp in x] for x in str_get_subchar(string)]\n","\n","\n","# Get Begin Middle End Single sequence\n","# B 0 M 1 E 2 S 3\n","\n","def str_bmes_idx(tokenList):\n","    answer = []\n","    for item in tokenList:\n","        if len(item) == 0:\n","            raise NameErro(\"Zero Length Word\")\n","        if len(item) == 1:\n","            answer.append(3)\n","        else:\n","            answer.append(0)\n","            for item in range(len(item) - 2):\n","                answer.append(1)\n","            answer.append(2)\n","    return answer\n","\n","def extract_features(data_set, isTest=False):\n","    data_temp = data_set.copy()\n","\n","    # Idx for chars\n","    with Pool(8) as p:\n","        tokinds = p.map(str_token_index,data_temp['raws'])\n","#     tokinds = [list(map(token_index, u)) for u in data_temp['raws']]\n","\n","    with Pool(8) as p:\n","        redinds = p.map(str_red_index,data_temp['raws'])\n","        \n","    with Pool(8) as p:\n","        subcharidx = p.map(str_subchar_index,data_temp['raws'])\n","        \n","\n","    data_temp[\"tokenIdx\"] = tokinds\n","    data_temp[\"redIdx\"] = redinds\n","    data_temp[\"subcharIdx\"] = subcharidx\n","    \n","    # BIO\n","    if(not isTest):\n","        data_temp[\"bmes\"] = [str_bmes_idx(u) for u in data_temp['tokenList']]\n","        assert (list(map(len,data_temp[\"bmes\"])) == list(map(len,data_temp[\"tokenIdx\"])))\n","\n","        \n","#     print(data_temp[\"bmes\"])\n","#    assert reduce(operator.and_, list(map(len,data_temp[\"bmes\"])) == list(map(len,data_temp[\"tokenIdx\"])))\n","    \n","    # Get redical\n","    \n","\n","    \n","    \n","    \n","    \n","    return data_temp\n","#     txt['token_indices'] = tokinds\n","#     if not istest:  # can't do this with the test set\n","#         bioints = [bio_index(b) for b in txt['bio_only']]\n","#         txt['bio_only'] = bioints\n","#     return txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4698\n","235\n","1328\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_OOs5BwB5tJY","executionInfo":{"status":"ok","timestamp":1610461470766,"user_tz":0,"elapsed":162071,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"67af88f2-74f4-4246-dadf-597ca407fca2"},"source":["str_red_index(\"花丸晴琉\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[182, 207, 60, 90]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"D9pCNUgW5tJZ","executionInfo":{"status":"ok","timestamp":1610461699278,"user_tz":0,"elapsed":390331,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"bf0e561d-2eae-4d82-abdb-60f13af2b3b9"},"source":["%%time\n","# MultiThreading\n","train_feature = extract_features(train)\n","# train_feature.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 4.48 s, sys: 523 ms, total: 5 s\n","Wall time: 3min 48s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":97},"id":"nO8a4CpB5tJa","executionInfo":{"status":"ok","timestamp":1610461699280,"user_tz":0,"elapsed":390113,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"8656ec98-3d29-4977-f046-2d04e1d65a53"},"source":["train_feature.head(1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>raws</th>\n","      <th>tokenList</th>\n","      <th>tokenIdx</th>\n","      <th>redIdx</th>\n","      <th>subcharIdx</th>\n","      <th>bmes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...</td>\n","      <td>迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）</td>\n","      <td>[迈向, 充满, 希望, 的, 新, 世纪, ——, 一九九八年, 新年, 讲话, （, 附...</td>\n","      <td>[2775, 4641, 873, 1797, 1428, 2458, 118, 1570,...</td>\n","      <td>[159, 208, 105, 170, 225, 49, 53, 92, 222, 98,...</td>\n","      <td>[[410, 1149], [1279, 116, 1291], [932, 524], [...</td>\n","      <td>[0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 2, 0, 1, 1, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               input  ...                                               bmes\n","0  迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...  ...  [0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 2, 0, 1, 1, ...\n","\n","[1 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"BrerrDcA5tJa"},"source":["def find_longest_sequence(data_with_features):\n","#     assert (np.max(list(map(len, data_with_features[\"tokenIdx\"])))) == (np.max(list(map(len, data_with_features[\"bmes\"]))))\n","    return (np.max(list(map(len, data_with_features[\"tokenIdx\"]))))\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ng7nxG7W5tJb","executionInfo":{"status":"ok","timestamp":1610461721212,"user_tz":0,"elapsed":411573,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"071312eb-d68d-449a-b80a-f36daf036bc6"},"source":["%%time\n","temp = extract_features(test, isTest=True)\n","test_feature = temp\n","# test_feature.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 215 ms, sys: 255 ms, total: 471 ms\n","Wall time: 21.9 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"xnhNz0me5tJb","executionInfo":{"status":"ok","timestamp":1610461721214,"user_tz":0,"elapsed":411305,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"c38558e1-ddc3-49d0-c91a-69f891c7d4ec"},"source":["test_feature.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>raws</th>\n","      <th>tokenIdx</th>\n","      <th>redIdx</th>\n","      <th>subcharIdx</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n","      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n","      <td>[678, 2926, 997, 63, 4686, 1681, 118, 1570, 37...</td>\n","      <td>[185, 208, 160, 159, 47, 200, 53, 92, 222, 98,...</td>\n","      <td>[[8, 285], [116, 651, 1291], [39, 959], [410, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>（二○○○年十二月三十一日）（附图片1张）</td>\n","      <td>（二○○○年十二月三十一日）（附图片1张）</td>\n","      <td>[1421, 1514, 4380, 4380, 4380, 1110, 3999, 151...</td>\n","      <td>[28, 76, 28, 28, 28, 196, 184, 76, 49, 222, 18...</td>\n","      <td>[[352], [651, 651], [989], [989], [989], [971,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>女士们，先生们，同志们，朋友们：</td>\n","      <td>女士们，先生们，同志们，朋友们：</td>\n","      <td>[4366, 1178, 3189, 50, 2068, 3562, 3189, 50, 2...</td>\n","      <td>[200, 42, 103, 28, 105, 83, 103, 28, 208, 67, ...</td>\n","      <td>[[108, 651, 971], [608, 651], [533, 41], [68],...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...</td>\n","      <td>2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...</td>\n","      <td>[1397, 4424, 4424, 4447, 1110, 1570, 1110, 205...</td>\n","      <td>[28, 28, 28, 28, 196, 92, 196, 54, 42, 8, 113,...</td>\n","      <td>[[328], [1035], [1035], [1058], [971, 876, 127...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...</td>\n","      <td>在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...</td>\n","      <td>[197, 613, 1553, 2914, 3055, 3941, 2352, 118, ...</td>\n","      <td>[23, 159, 193, 170, 118, 133, 67, 53, 60, 160,...</td>\n","      <td>[[651, 533, 693], [410, 506], [533, 297], [947...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               input  ...                                         subcharIdx\n","0                              共同创造美好的新世纪——二○○一年新年贺词  ...  [[8, 285], [116, 651, 1291], [39, 959], [410, ...\n","1                              （二○○○年十二月三十一日）（附图片1张）  ...  [[352], [651, 651], [989], [989], [989], [971,...\n","2                                   女士们，先生们，同志们，朋友们：  ...  [[108, 651, 971], [608, 651], [533, 41], [68],...\n","3  2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...  ...  [[328], [1035], [1035], [1058], [971, 876, 127...\n","4  在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...  ...  [[651, 533, 693], [410, 506], [533, 297], [947...\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqIxdbWw5tJc","executionInfo":{"status":"ok","timestamp":1610461721215,"user_tz":0,"elapsed":411057,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"8f23d824-e23a-4dbb-fc3c-d15b957ebed2"},"source":["\n","train_longest = find_longest_sequence(train_feature)\n","print(train_longest)\n","\n","test_longest = find_longest_sequence(test_feature)\n","print(test_longest)\n","\n","seq_longest = np.max([train_longest,test_longest])\n","print(seq_longest)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1019\n","626\n","1019\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IgaS2m4c5tJd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnmw5YC65tJd"},"source":["# find out max sub char length \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4D-oMGF5tJd"},"source":["subchar_seq_length = np.max(list(map(lambda x : len(getSubChar(x)),train_dic_char)))\n","subchar_seq_length = 5\n","subsubchar_padtok = oov_subchar + 1\n","seq_length = seq_longest\n","\n","def padd_char(seq):\n","    temp_char_seqs_padded = []\n","    for item in seq[\"subcharIdx\"]:\n","        temp_pad = pad_sequences(item, maxlen=subchar_seq_length,\n","                                  dtype='int32', padding='post', truncating='post', value=subsubchar_padtok)\n","  \n","        a = temp_pad\n","        b = [[subsubchar_padtok for i in range(subchar_seq_length)] for _ in range(0, seq_length - len(temp_pad))]\n","        if len(b) == 0:\n","            c = a\n","        else:\n","            c = np.concatenate((a, b))\n","        \n","    # print(len(c))\n","        temp_char_seqs_padded.append(c)\n","  # print(len(temp_char_seqs_padded))\n","    return temp_char_seqs_padded"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EItM6VIu5tJe"},"source":["train_subchar_seqs_padded = padd_char(train_feature)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnwZd2HK5tJe","executionInfo":{"status":"ok","timestamp":1610461793674,"user_tz":0,"elapsed":480932,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"dbe149ad-4625-4037-8107-f0a5883b5f6b"},"source":["seq_length = seq_longest\n","\n","# a new dummy token index, one more than OOV\n","padtok = oov+1\n","red_padtok = oov_redical + 1\n","print('The padding token index is %i' % padtok)\n","\n","# use pad_sequences, padding or truncating at the end of the sequence (default is 'pre')\n","train_seqs_padded = pad_sequences(train_feature['tokenIdx'].tolist(), maxlen=seq_length,\n","                                  dtype='int32', padding='post', truncating='post', value=padtok)\n","print('Example of padded token sequence:')\n","print(train_seqs_padded[1])\n","\n","\n","train_red_padded = pad_sequences(train_feature['redIdx'].tolist(), maxlen=seq_length,\n","                                  dtype='int32', padding='post', truncating='post', value=red_padtok)\n","\n","\n","train_subchar_seqs_padded = padd_char(train_feature)\n","\n","\n","\n","\n","\n","# Prepare Test set.\n","test_seqs_padded = pad_sequences(test_feature['tokenIdx'].tolist(), maxlen=seq_length,\n","                                  dtype='int32', padding='post', truncating='post', value=padtok)\n","\n","test_rad_padded = pad_sequences(test_feature['redIdx'].tolist(), maxlen=seq_length,\n","                                  dtype='int32', padding='post', truncating='post', value=red_padtok)\n","\n","test_subchar_seqs_padded = padd_char(test_feature)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The padding token index is 4699\n","Example of padded token sequence:\n","[4192  678 4192 ... 4699 4699 4699]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2KdqtDWG5tJf"},"source":["from keras.utils import to_categorical\n","\n","# get lists of named entity labels, padded with a null label (=3)\n","padlab = 4\n","train_labs_padded = pad_sequences(train_feature['bmes'].tolist(), maxlen=seq_length,\n","                                  dtype='int32', padding='post', truncating='post', value=padlab)\n","\n","# convert those labels to one-hot encoding\n","n_labs = 5\n","train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]\n","\n","# # follow the print outputs below to see how the labels are transformed\n","# print('Length of input sequence: %i' % len(train_labs_padded[1]))\n","# print('Length of label sequence: %i' % len(train_labs_onehot[1]))\n","# print(train_labs_padded[1][:11])\n","# print(train_labs_onehot[1][:11])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YomS33Me5tJf","executionInfo":{"status":"ok","timestamp":1610461797674,"user_tz":0,"elapsed":483598,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"417f8ad2-7b9d-4b88-8cfa-addb0b3d7828"},"source":["# load Keras and TensorFlow\n","\n","\n","\n","# our final vocab size is the padding token + 1 (OR length of vocab + OOV + PAD)\n","vocab_size = padtok+1\n","red_size = red_padtok+1\n","subchar_size = subsubchar_padtok + 1\n","\n","print(vocab_size==len(train_dic_char)+2)\n","embed_size = 128 # y an embedding size of 128 (could tune this)\n","\n","# list of metrics to use: true & false positives, negatives, accuracy, precision, recall, area under the curve\n","METRICS = [\n","      keras.metrics.TruePositives(name='tp'),\n","      keras.metrics.FalsePositives(name='fp'),\n","      keras.metrics.TrueNegatives(name='tn'),\n","      keras.metrics.FalseNegatives(name='fn'), \n","      keras.metrics.BinaryAccuracy(name='accuracy'),\n","      keras.metrics.Precision(name='precision'),\n","      keras.metrics.Recall(name='recall'),\n","      keras.metrics.AUC(name='auc'),\n","]\n","\n","# our model has the option for an label prediction bias, it's sequential, starts with an embedding layer, then bi-LSTM,\n","# a dropout layer follows for regularisation, and a dense final layer with softmax activation to output class probabilities\n","# we compile with the Adam optimizer at a low learning rate, use categorical cross-entropy as our loss function\n","def make_model(metrics = METRICS, output_bias=None):\n","    if output_bias is not None:        \n","        output_bias = tf.keras.initializers.Constant(output_bias)\n","    tok_input1 = keras.layers.Input(shape=(seq_length,), dtype='int32', name='tok_input1')\n","    red_input2 = keras.layers.Input(shape=(seq_length,), dtype='int32', name='red_input2')\n","    subchar_input3 = keras.layers.Input(shape=(seq_length,subchar_seq_length), dtype='int32', name='char_input3')\n","\n","    emb_char = keras.layers.TimeDistributed(keras.layers.Embedding(output_dim=embed_size, input_dim=subchar_size, input_length=5,  mask_zero=True, trainable=True))(subchar_input3)\n","    char_enc = keras.layers.TimeDistributed(keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=False, dropout=0.2)))(emb_char)\n","        \n","\n","    x1 = keras.layers.Embedding(output_dim=embed_size, input_dim=vocab_size,  input_length=seq_length,  mask_zero=True, trainable=True)(tok_input1)\n","    x2 = keras.layers.Embedding(output_dim=embed_size, input_dim=red_size,  input_length=seq_length, mask_zero=True, trainable=True)(red_input2)\n","    x_cancat = keras.layers.concatenate([x1, x2, char_enc])\n","#     x_cancat = (x1)\n","    x_lstm = keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2))(x_cancat)\n","    x_drop = keras.layers.Dropout(0.5)(x_lstm)\n","    main_output = keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias))(x_drop)\n","    model = keras.models.Model(inputs=[tok_input1,red_input2, subchar_input3], outputs= main_output)\n","#     model = keras.models.Model(inputs=[tok_input1, pos_input2, char_input3], outputs= main_output)\n","    \n","    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n","    return model\n","\n","\n","# Old lstm model\n","#     model = keras.Sequential([\n","#         keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True),\n","#         keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2)),  # 2 directions, 50 units each, concatenated (can change this)\n","#         keras.layers.Dropout(0.5),\n","#         keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias)),\n","#     ])\n","\n","\n","\n","\n","\n","\n","# early stopping criteria based on area under the curve: will stop if no improvement after 10 epochs\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n","\n","# the number of training epochs we'll use, and the batch size (how many texts are input at once)\n","EPOCHS = 100\n","BATCH_SIZE = 128\n","\n","print('**Defining a neural network**')\n","model = make_model()\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","**Defining a neural network**\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","char_input3 (InputLayer)        [(None, 1019, 5)]    0                                            \n","__________________________________________________________________________________________________\n","tok_input1 (InputLayer)         [(None, 1019)]       0                                            \n","__________________________________________________________________________________________________\n","red_input2 (InputLayer)         [(None, 1019)]       0                                            \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, 1019, 5, 128) 170240      char_input3[0][0]                \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 1019, 128)    601600      tok_input1[0][0]                 \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 1019, 128)    30336       red_input2[0][0]                 \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 1019, 100)    71600       time_distributed[0][0]           \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 1019, 356)    0           embedding_1[0][0]                \n","                                                                 embedding_2[0][0]                \n","                                                                 time_distributed_1[0][0]         \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 1019, 100)    162800      concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 1019, 100)    0           bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, 1019, 5)      505         dropout[0][0]                    \n","==================================================================================================\n","Total params: 1,037,081\n","Trainable params: 1,037,081\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lBCHmvJ-5tJh"},"source":["# %%time\n","# # evaluate our initial model\n","# results = model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=0)\n","# print(\"Loss: {:0.4f}\".format(results[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8oVO_AL5tJi","executionInfo":{"status":"ok","timestamp":1610461800532,"user_tz":0,"elapsed":480285,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"4d22ce9e-2617-4245-c7e5-245d3193a9cb"},"source":["# figure out the label distribution in our fixed-length texts\n","from collections import Counter\n","all_labs = [l for lab in train_labs_padded for l in lab]\n","label_count = Counter(all_labs)\n","total_labs = len(all_labs)\n","print(label_count)\n","print(total_labs)\n","\n","# use this to define an initial model bias\n","initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n","              (label_count[2]/total_labs), (label_count[3]/total_labs), (label_count[4]/total_labs)]\n","print('Initial bias:')\n","print(initial_bias)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Counter({4: 17589578, 0: 585226, 2: 585226, 3: 524721, 1: 131275})\n","19416026\n","Initial bias:\n","[0.03014138938627297, 0.006761167295511451, 0.03014138938627297, 0.027025149224666263, 0.9059309047072763]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQunOMCM5tJj","executionInfo":{"status":"ok","timestamp":1610461801470,"user_tz":0,"elapsed":478899,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"6e931269-5820-4473-b8a2-7796448446b7"},"source":["# prepare sequences and labels as numpy arrays, check dimensions\n","X = np.array(train_seqs_padded)\n","X_red = np.array(train_red_padded)\n","X_sub = np.array(train_subchar_seqs_padded)\n","y = np.array(train_labs_onehot)\n","print('Input sequence dimensions (n.docs, seq.length):')\n","print(X.shape)\n","print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n","print(y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input sequence dimensions (n.docs, seq.length):\n","(19054, 1019)\n","Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n","(19054, 1019, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":484},"id":"eC1_C2qJ5tJj","executionInfo":{"status":"error","timestamp":1610461820309,"user_tz":0,"elapsed":18833,"user":{"displayName":"Hu S","photoUrl":"","userId":"01703764064702421195"}},"outputId":"110161d4-35cc-4160-b23a-070ebbca192b"},"source":["# re-initiate model with bias\n","model = make_model(output_bias=initial_bias)\n","# and fit...\n","model.fit([X,X_red,X_sub], y, batch_size=32, epochs=10, callbacks = [early_stopping],  validation_split = 0.3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n"],"name":"stdout"},{"output_type":"error","ename":"CancelledError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-02705282b7df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# and fit...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_red\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_sub\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCancelledError\u001b[0m:  [_Derived_]RecvAsync is cancelled.\n\t [[{{node broadcast_weights_7/assert_broadcastable/is_valid_shape/else/_307/broadcast_weights_7/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/then/_904/broadcast_weights_7/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/_684}}]] [Op:__inference_train_function_38340]\n\nFunction call stack:\ntrain_function\n"]}]},{"cell_type":"code","metadata":{"id":"W604J1XP5tJk"},"source":["X_test = np.array(test_seqs_padded)\n","X_test_rad = np.array(test_rad_padded)\n","# preds = np.argmax(model.predict(X_test), axis=-1)\n","preds = np.argmax(model.predict([X_test,X_test_rad]), axis=-1)\n","flat_preds = [p for pred in preds for p in pred]\n","print(Counter(flat_preds))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DysEPJN95tJk"},"source":["len(preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7m60RNlX5tJk"},"source":["def postEditPred(pred):\n","    print(pred)\n","    ans = []\n","    for item in pred:\n","        if item == 0 or item == 3:\n","            ans.append(1)\n","        else:\n","            ans.append(0)\n","            \n","    assert len(ans) == len(pred)\n","    return ans\n","\n","def splitSentence(st, pred):\n","    temp_st = st\n","    temp_pred = pred\n","    temp_pred[0] = 2\n","    buf = []\n","    result = []\n","    for (pre,char) in zip(temp_pred,temp_st):\n","        if(pre == 0 or pre == 3):\n","            result.append(buf)\n","            buf = []\n","        buf.append(char)\n","    if(len(buf) > 0):\n","        result.append(buf)\n","    return result\n","    \n","    \n","\n","\n","\n","def sentencePrediction(dataset,prediction):\n","    data_temp = dataset.copy()\n","    assert len(data_temp) == len(prediction)\n","    raw_sents = data_temp[\"raws\"]\n","    assert len(raw_sents) == len(prediction)\n","    ans = []\n","    for (st,pred) in zip(raw_sents,prediction):\n","        sptSt = splitSentence(st,pred)\n","        ans.append(sptSt)\n","    assert len(ans) == len(data_temp)\n","    data_temp[\"tokenList\"]= ans\n","    return data_temp\n","    \n","def convertToPredSts(dataset):\n","    data_temp = dataset.copy()\n","    tokenList = data_temp[\"tokenList\"]\n","    ans = []\n","    \n","    for item in tokenList:\n","        temp_st = list(map(lambda x : \"\".join(x), item))\n","        temp = \"  \".join(temp_st)\n","        temp = temp + \"  \"\n","        ans.append(temp)\n","    return ans"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NoszAJKS5tJl"},"source":["result_pred = sentencePrediction(test_feature,preds)\n","output_sts = convertToPredSts(result_pred)\n","\n","f = open(\"./ans.txt\", \"w\")\n","f.write(\"\\n\".join(output_sts))\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ov-lmmDI5tJl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8VXykZpc5tJm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHS1c1Hv5tJm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLbN_VYv5tJm"},"source":[""],"execution_count":null,"outputs":[]}]}