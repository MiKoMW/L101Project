{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the google colab\n",
    "# !pip install xmnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "isColab = False\n",
    "googlePath = \"./drive/MyDrive/L101Project/\" if isColab else \"./\"\n",
    "cpuPools = 2 if isColab else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import Session\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = Session(config=config)\n",
    "# tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import xmnlp\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerTrain = googlePath + \"./corpus/ner/ner_train.txt\"\n",
    "nerDev = googlePath + \"./corpus/ner/ner_dev.txt\"\n",
    "nerTest = googlePath + \"./corpus/ner/ner_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_table(nerTrain, header=None, names=['token', 'label'])  # don't drop the empty lines yet, they show up as NaN in the data frame\n",
    "dev = pd.read_table(nerDev, header=None, names=['token', 'label'])  # don't drop the empty lines yet, they show up as NaN in the data frame\n",
    "test = pd.read_table(nerTest, header=None, names=['token', 'label'])  # don't drop the empty lines yet, they show up as NaN in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>海</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token label\n",
       "0     海     O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O', nan}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>token_indices</th>\n",
       "      <th>bio</th>\n",
       "      <th>bio_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>海</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>钓</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>比</td>\n",
       "      <td>O</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>赛</td>\n",
       "      <td>O</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>地</td>\n",
       "      <td>O</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>点</td>\n",
       "      <td>O</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>在</td>\n",
       "      <td>O</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>厦</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>门</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>与</td>\n",
       "      <td>O</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>金</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>门</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>之</td>\n",
       "      <td>O</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>间</td>\n",
       "      <td>O</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>的</td>\n",
       "      <td>O</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>海</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>域</td>\n",
       "      <td>O</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>。</td>\n",
       "      <td>O</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>这</td>\n",
       "      <td>O</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>座</td>\n",
       "      <td>O</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>依</td>\n",
       "      <td>O</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>山</td>\n",
       "      <td>O</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>傍</td>\n",
       "      <td>O</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>水</td>\n",
       "      <td>O</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>的</td>\n",
       "      <td>O</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>博</td>\n",
       "      <td>O</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>物</td>\n",
       "      <td>O</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>馆</td>\n",
       "      <td>O</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>由</td>\n",
       "      <td>O</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token  label  token_indices  bio  bio_only\n",
       "0      海      O            0.0  6.0       2.0\n",
       "1      钓      O            1.0  6.0       2.0\n",
       "2      比      O            2.0  6.0       2.0\n",
       "3      赛      O            3.0  6.0       2.0\n",
       "4      地      O            4.0  6.0       2.0\n",
       "5      点      O            5.0  6.0       2.0\n",
       "6      在      O            6.0  6.0       2.0\n",
       "7      厦  B-LOC            7.0  0.0       0.0\n",
       "8      门  I-LOC            8.0  3.0       1.0\n",
       "9      与      O            9.0  6.0       2.0\n",
       "10     金  B-LOC           10.0  0.0       0.0\n",
       "11     门  I-LOC            8.0  3.0       1.0\n",
       "12     之      O           11.0  6.0       2.0\n",
       "13     间      O           12.0  6.0       2.0\n",
       "14     的      O           13.0  6.0       2.0\n",
       "15     海      O            0.0  6.0       2.0\n",
       "16     域      O           14.0  6.0       2.0\n",
       "17     。      O           15.0  6.0       2.0\n",
       "18   NaN    NaN            NaN  NaN       NaN\n",
       "19     这      O           17.0  6.0       2.0\n",
       "20     座      O           18.0  6.0       2.0\n",
       "21     依      O           19.0  6.0       2.0\n",
       "22     山      O           20.0  6.0       2.0\n",
       "23     傍      O           21.0  6.0       2.0\n",
       "24     水      O           22.0  6.0       2.0\n",
       "25     的      O           13.0  6.0       2.0\n",
       "26     博      O           23.0  6.0       2.0\n",
       "27     物      O           24.0  6.0       2.0\n",
       "28     馆      O           25.0  6.0       2.0\n",
       "29     由      O           26.0  6.0       2.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label translation and token idx\n",
    "\n",
    "token_vocab = train.token.unique().tolist()\n",
    "oov = len(token_vocab) \n",
    "\n",
    "def token_index(tok):\n",
    "  ind = tok\n",
    "  if not pd.isnull(tok):  # new since last time: deal with the empty lines which we didn't drop yet\n",
    "    if tok in token_vocab:  # if token in vocabulary\n",
    "      ind = token_vocab.index(tok)\n",
    "    else:  # else it's OOV\n",
    "      ind = oov\n",
    "  return ind\n",
    "\n",
    "def bio_index(bio):\n",
    "  ind = bio\n",
    "  if not pd.isnull(bio):  # deal with empty lines\n",
    "    if bio=='B-LOC':\n",
    "      ind = 0\n",
    "    elif bio=='B-ORG':\n",
    "      ind = 1\n",
    "    elif bio=='B-PER':\n",
    "      ind = 2\n",
    "    elif bio=='I-LOC':\n",
    "      ind = 3\n",
    "    elif bio=='I-ORG':\n",
    "      ind = 4\n",
    "    elif bio=='I-PER':\n",
    "      ind = 5\n",
    "    elif bio=='O':\n",
    "      ind = 6\n",
    "  return ind\n",
    "\n",
    "\n",
    "def bio_only_idx(bio):\n",
    "  ind = bio\n",
    "  if not pd.isnull(bio):  # deal with empty lines\n",
    "    if bio=='B-LOC':\n",
    "      ind = 0\n",
    "    elif bio=='B-ORG':\n",
    "      ind = 0\n",
    "    elif bio=='B-PER':\n",
    "      ind = 0\n",
    "    elif bio=='I-LOC':\n",
    "      ind = 1\n",
    "    elif bio=='I-ORG':\n",
    "      ind = 1\n",
    "    elif bio=='I-PER':\n",
    "      ind = 1\n",
    "    elif bio=='O':\n",
    "      ind = 2\n",
    "  return ind\n",
    "\n",
    "# pass a data frame through our feature extractor\n",
    "def extract_features(txt_orig,istest=False):\n",
    "  txt = txt_orig.copy()\n",
    "  tokinds = [token_index(u) for u in txt['token']]\n",
    "  txt['token_indices'] = tokinds\n",
    "  if not istest:  # can't do this with the test set\n",
    "    bioints = [bio_index(b) for b in txt['label']]\n",
    "    bioints_only = [bio_only_idx(b) for b in txt['label']]\n",
    "    txt['bio'] = bioints\n",
    "    txt['bio_only'] = bioints_only\n",
    "\n",
    "    return txt\n",
    "\n",
    "train_copy = extract_features(train)\n",
    "train_copy.head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cell takes a little while to run: be patient :)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_num</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>token_indices</th>\n",
       "      <th>bio</th>\n",
       "      <th>bio_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOC, I-LOC, O, B-LOC, ...</td>\n",
       "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 0.0, 3.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence_num                                              token  \\\n",
       "0             0  [海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...   \n",
       "\n",
       "                                               label  \\\n",
       "0  [O, O, O, O, O, O, O, B-LOC, I-LOC, O, B-LOC, ...   \n",
       "\n",
       "                                       token_indices  \\\n",
       "0  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
       "\n",
       "                                                 bio  \\\n",
       "0  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 0.0, 3.0, ...   \n",
       "\n",
       "                                            bio_only  \n",
       "0  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens2sequences(txt_orig,istest=False):\n",
    "\n",
    "    txt = txt_orig.copy()\n",
    "    txt['sequence_num'] = 0\n",
    "    seqcount = 0\n",
    "    for i in txt.index:  # in each row...\n",
    "        txt.loc[i,'sequence_num'] = seqcount  # set the sequence number\n",
    "        if pd.isnull(txt.loc[i,'token']):  # increment sequence counter at empty lines\n",
    "            seqcount += 1\n",
    "    # now drop the empty lines, group by sequence number and output df of sequence lists\n",
    "    txt = txt.dropna()\n",
    "    if istest:  # test set doesn't have labels\n",
    "        txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'label', 'token_indices', 'bio', \"bio_only\"]].agg(lambda x: list(x))\n",
    "    else:\n",
    "        txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'label', 'token_indices', 'bio', \"bio_only\"]].agg(lambda x: list(x))\n",
    "    return txt_seqs\n",
    "\n",
    "print(\"This cell takes a little while to run: be patient :)\")\n",
    "train_seqs = tokens2sequences(train_copy)\n",
    "train_seqs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_copy = extract_features(dev)\n",
    "dev_seqs = tokens2sequences(dev_copy)\n",
    "test_copy = extract_features(test)\n",
    "test_seqs = tokens2sequences(test_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_seqs.to_csv(\"./trains_seq.csv\", sep='\\t', encoding='utf-8')\n",
    "# dev_seqs.to_csv(\"./dev_seqs.csv\", sep='\\t', encoding='utf-8')\n",
    "# test_seqs.to_csv(\"./test_seq.csv\", sep='\\t', encoding='utf-8')\n",
    "from ast import literal_eval\n",
    "train_seqs_temp = pd.read_table(\"./trains_seq.csv\",converters={\"token\": literal_eval,\n",
    "                                                               \"token_indices\": literal_eval,\n",
    "                                                               \"label\": literal_eval,\n",
    "                                                               \"bio\": literal_eval,\n",
    "                                                               \"bio_only\": literal_eval})\n",
    "test_seqs_temp = pd.read_table(\"./test_seq.csv\",converters={\"token\": literal_eval,\n",
    "                                                               \"token_indices\": literal_eval,\n",
    "                                                               \"label\": literal_eval,\n",
    "                                                               \"bio\": literal_eval,\n",
    "                                                               \"bio_only\": literal_eval}) \n",
    "dev_seqs_temp = pd.read_table(\"./dev_seqs.csv\",converters={\"token\": literal_eval,\n",
    "                                                               \"token_indices\": literal_eval,\n",
    "                                                               \"label\": literal_eval,\n",
    "                                                               \"bio\": literal_eval,\n",
    "                                                               \"bio_only\": literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Corpus for Chinese Word Segamentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkuTrain = googlePath + \"./corpus/ner/ner_train.txt\"\n",
    "# pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "\n",
    "# f = open(pkuTest, \"r\")\n",
    "# temp = f.readlines()\n",
    "# con = 0\n",
    "# for item in temp:\n",
    "#     con+=1\n",
    "#     if(item == \"\\n\"):\n",
    "#         print(con)\n",
    "\n",
    "# import pandas as pd\n",
    "# pkuTrain = googlePath + \"./corpus/cws/icwb2-data/training/pku_training.utf8\"\n",
    "# pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "# train = pd.read_table(pkuTrain,  encoding='utf8', header=None, names=['input'])  # don't drop the empty lines yet, they show up as NaN in the data frame\n",
    "# # train.head(n=1)\n",
    "\n",
    "# pkuTest = googlePath  + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "# test = pd.read_table(pkuTest,  encoding='utf8', header=None, names=['input']) \n",
    "# test[\"raws\"] = test[\"input\"]\n",
    "# # test.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def postPorcess(dataset):\n",
    "#     data_temp = dataset.copy()\n",
    "#     tokens = data_temp[\"token\"]\n",
    "#     data_temp[\"raws\"] \n",
    "# #     print(tokens)\n",
    "#     print(\"\".join(tokens))\n",
    "\n",
    "# postPorcess(train_seqs_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20864\n",
      "2318\n",
      "4636\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def prepross_data(train_Data):\n",
    "    inputs = train_Data[\"token\"][:]\n",
    "    print(len(inputs))\n",
    "    raws = []\n",
    "    tokenList = []\n",
    "    \n",
    "    for item in inputs:\n",
    "        tokenList.append(item)\n",
    "        item = \"\".join(item)\n",
    "        raws.append(item)\n",
    "        tokens = (item)\n",
    "    train_Data[\"raws\"] = raws\n",
    "    train_Data[\"tokenList\"] = tokenList\n",
    "    dictionary_train_word = list(set(reduce(operator.add,train_Data[\"tokenList\"])))\n",
    "    dictionary_train_char =  list(set(reduce(operator.add,train_Data[\"raws\"])))    \n",
    "    return train_Data, dictionary_train_char, dictionary_train_word\n",
    "\n",
    "train, train_dic_char, train_dic_word  = prepross_data(train_seqs_temp)\n",
    "dev, _, _  = prepross_data(dev_seqs_temp)\n",
    "test, _, _  = prepross_data(test_seqs_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sequence_num</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>token_indices</th>\n",
       "      <th>bio</th>\n",
       "      <th>bio_only</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOC, I-LOC, O, B-LOC, ...</td>\n",
       "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 0.0, 3.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, ...</td>\n",
       "      <td>海钓比赛地点在厦门与金门之间的海域。</td>\n",
       "      <td>[海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sequence_num  \\\n",
       "0           0             0   \n",
       "\n",
       "                                               token  \\\n",
       "0  [海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...   \n",
       "\n",
       "                                               label  \\\n",
       "0  [O, O, O, O, O, O, O, B-LOC, I-LOC, O, B-LOC, ...   \n",
       "\n",
       "                                       token_indices  \\\n",
       "0  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
       "\n",
       "                                                 bio  \\\n",
       "0  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 0.0, 3.0, ...   \n",
       "\n",
       "                                            bio_only                raws  \\\n",
       "0  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, ...  海钓比赛地点在厦门与金门之间的海域。   \n",
       "\n",
       "                                           tokenList  \n",
       "0  [海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lazy Load) Loading model...\n",
      "235\n"
     ]
    }
   ],
   "source": [
    "# Get redical dictionary\n",
    "\n",
    "# Get redical of a chinese character:\n",
    "def str_get_redical(st):\n",
    "    return xmnlp.radical(st)\n",
    "\n",
    "def get_redical_dic_from_char_dic(char_dic):\n",
    "    return list(set(str_get_redical(\"\".join(char_dic))))\n",
    "\n",
    "train_dic_redical = get_redical_dic_from_char_dic(train_dic_char)\n",
    "print(len(train_dic_redical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up subcharacter dictionary\n",
    "# Build up Chinese sub-character unites diactionary.\n",
    "chaiZi_Dic = {}\n",
    "with open(googlePath + \"corpus/chaizi/chaizi-jt.txt\", 'r') as f:\n",
    "    reader = csv.reader(f,delimiter='\\t')\n",
    "    for row in reader:\n",
    "        chaiZi_Dic[row[0]] = row[1]\n",
    "        \n",
    "def getSubChar(charCN):\n",
    "    if(charCN in chaiZi_Dic.keys()):\n",
    "        return chaiZi_Dic[charCN].split(\" \")\n",
    "#     elif charCN in train_dic_char:\n",
    "#         return [charCN]\n",
    "    else:\n",
    "        return [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142\n"
     ]
    }
   ],
   "source": [
    "def str_get_subchar(st):\n",
    "#     for item in st:\n",
    "#         print(item)\n",
    "#         print(getSubChar(item))\n",
    "    return [getSubChar(item) for item in st]\n",
    "\n",
    "def get_subchar_dic_from_char_dic(char_dic):\n",
    "    return list(set(itertools.chain.from_iterable(str_get_subchar(\"\".join(char_dic)))))\n",
    "\n",
    "\n",
    "def get_top200_subchar_dic_from_char_dic(char_dic):\n",
    "    allsub = (list(itertools.chain.from_iterable(str_get_subchar(\"\".join(char_dic)))))\n",
    "    c = Counter(allsub)\n",
    "    ans = list(map(lambda x : x[0] , (c.most_common(200))))\n",
    "    return ans\n",
    "# train_dic_subchar = get_subchar_dic_from_char_dic(train_dic_char)\n",
    "train_dic_subchar_top200 = get_top200_subchar_dic_from_char_dic(train_dic_char)\n",
    "\n",
    "\n",
    "train_dic_subchar = get_subchar_dic_from_char_dic(train_dic_char)\n",
    "print(len(train_dic_subchar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the pre-processing\n",
    "def validate_data(data):\n",
    "    tkList = data[\"tokenList\"]\n",
    "    raws = data[\"raws\"]\n",
    "    for (tokens, raw) in zip(tkList,raws):\n",
    "#         print(tokens)\n",
    "        \n",
    "#         print(raw)\n",
    "        temp  = \"\".join(tokens)\n",
    "#         print(temp)\n",
    "        assert(temp == raw)\n",
    "        if not (temp == raw):\n",
    "            print(temp)\n",
    "            print(raw)\n",
    "            print(tokens)\n",
    "validate_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_dic_char))\n",
    "# print(len(set(train_dic_char)))\n",
    "# print(len(train_dic_word))\n",
    "# print(len(set(train_dic_word)))\n",
    "# with open(googlePath + \"./corpus/cws/icwb2-data/gold/pku_training_words.utf8\", 'r') as f:\n",
    "#     content = f.readlines()  \n",
    "#     print(len(content))\n",
    "\n",
    "# Make bag of subchar model.\n",
    "\n",
    "\n",
    "# \tletter = \n",
    "# \tletter[value] = 1\n",
    "\n",
    "\n",
    "\n",
    "# str_to_bagOfChar(\"迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）\")[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4312\n",
      "235\n",
      "1142\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "\n",
    "oov = len(train_dic_char)\n",
    "print(oov)\n",
    "\n",
    "oov_redical = len(train_dic_redical)\n",
    "print(oov_redical)\n",
    "\n",
    "oov_subchar = len(train_dic_subchar)\n",
    "print(oov_subchar)\n",
    "\n",
    "def token_index(tok):\n",
    "    ind = tok\n",
    "    if tok in train_dic_char:  # if token in vocabulary\n",
    "        ind = train_dic_char.index(tok)\n",
    "    else:  # else it's OOV\n",
    "        ind = oov\n",
    "    return ind\n",
    "\n",
    "def str_token_index(string):\n",
    "    return [token_index(x) for x in (string)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def redical_index(red):\n",
    "    ind = oov_redical\n",
    "    if red in train_dic_redical:  # if token in vocabulary\n",
    "        ind = train_dic_redical.index(red)\n",
    "    return ind\n",
    "\n",
    "def str_red_index(string):\n",
    "    return [redical_index(x) for x in str_get_redical(string)]\n",
    "\n",
    "\n",
    "def subchar_index(subchar):\n",
    "    ind = oov_subchar\n",
    "    if subchar in train_dic_subchar:\n",
    "        ind = train_dic_subchar.index(subchar)\n",
    "    return ind\n",
    "\n",
    "def str_subchar_index(string):\n",
    "    return [[subchar_index(temp) for temp in x] for x in str_get_subchar(string)]\n",
    "\n",
    "def char_to_bagOfChar(cnChar):\n",
    "    ans = [0 for _ in range(len(train_dic_subchar_top200)+1)]\n",
    "    for item in (getSubChar(cnChar)):\n",
    "        if item in train_dic_subchar_top200:\n",
    "            idx = train_dic_subchar_top200.index(item)\n",
    "        else:\n",
    "            idx = len(train_dic_subchar_top200)\n",
    "        ans[idx] += 1\n",
    "    return (ans)\n",
    "\n",
    "def str_to_bagOfChar(cnStr):\n",
    "    return [char_to_bagOfChar(cnChar) for cnChar in cnStr]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# # Get Begin Middle End Single sequence\n",
    "# # B 0 M 1 E 2 S 3\n",
    "\n",
    "# def str_bmes_idx(tokenList):\n",
    "#     answer = []\n",
    "#     for item in tokenList:\n",
    "#         if len(item) == 0:\n",
    "#             raise NameErro(\"Zero Length Word\")\n",
    "#         if len(item) == 1:\n",
    "#             answer.append(3)\n",
    "#         else:\n",
    "#             answer.append(0)\n",
    "#             for item in range(len(item) - 2):\n",
    "#                 answer.append(1)\n",
    "#             answer.append(2)\n",
    "#     return answer\n",
    "\n",
    "def extract_features(data_set, isTest=False):\n",
    "    data_temp = data_set.copy()\n",
    "\n",
    "    # Idx for chars\n",
    "    with Pool(8) as p:\n",
    "        tokinds = p.map(str_token_index,data_temp['raws'])\n",
    "#     tokinds = [list(map(token_index, u)) for u in data_temp['raws']]\n",
    "\n",
    "    with Pool(8) as p:\n",
    "        redinds = p.map(str_red_index,data_temp['raws'])\n",
    "        \n",
    "    with Pool(8) as p:\n",
    "        subcharidx = p.map(str_subchar_index,data_temp['raws'])\n",
    "        \n",
    "#     subcharidx_bag = list(m0ap(str_to_bagOfChar,data_temp['raws']))\n",
    "        \n",
    "\n",
    "    data_temp[\"tokenIdx\"] = tokinds\n",
    "    data_temp[\"redIdx\"] = redinds\n",
    "    data_temp[\"subcharIdx\"] = subcharidx\n",
    "#     data_temp[\"subcharIdxBag\"] = subcharidx_bag\n",
    "    \n",
    "    # BIO\n",
    "#     if(not isTest):\n",
    "#         data_temp[\"bmes\"] = [str_bmes_idx(u) for u in data_temp['tokenList']]\n",
    "#         assert (list(map(len,data_temp[\"bmes\"])) == list(map(len,data_temp[\"tokenIdx\"])))\n",
    "    \n",
    "    return data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 s, sys: 256 ms, total: 1.79 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MultiThreading\n",
    "temp = extract_features(train)\n",
    "train_feature = temp.drop(columns=[\"token\",\"token_indices\"])\n",
    "# train_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sequence_num</th>\n",
       "      <th>label</th>\n",
       "      <th>bio</th>\n",
       "      <th>bio_only</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenList</th>\n",
       "      <th>tokenIdx</th>\n",
       "      <th>redIdx</th>\n",
       "      <th>subcharIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-LOC, I-LOC, O, B-LOC, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 0.0, 3.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, ...</td>\n",
       "      <td>海钓比赛地点在厦门与金门之间的海域。</td>\n",
       "      <td>[海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...</td>\n",
       "      <td>[166, 2518, 995, 4050, 1673, 2546, 4064, 790, ...</td>\n",
       "      <td>[121, 99, 156, 4, 131, 8, 131, 192, 201, 170, ...</td>\n",
       "      <td>[[527, 971], [566, 948], [55, 55], [810, 1021,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sequence_num  \\\n",
       "0           0             0   \n",
       "\n",
       "                                               label  \\\n",
       "0  [O, O, O, O, O, O, O, B-LOC, I-LOC, O, B-LOC, ...   \n",
       "\n",
       "                                                 bio  \\\n",
       "0  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 0.0, 3.0, ...   \n",
       "\n",
       "                                            bio_only                raws  \\\n",
       "0  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, ...  海钓比赛地点在厦门与金门之间的海域。   \n",
       "\n",
       "                                           tokenList  \\\n",
       "0  [海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...   \n",
       "\n",
       "                                            tokenIdx  \\\n",
       "0  [166, 2518, 995, 4050, 1673, 2546, 4064, 790, ...   \n",
       "\n",
       "                                              redIdx  \\\n",
       "0  [121, 99, 156, 4, 131, 8, 131, 192, 201, 170, ...   \n",
       "\n",
       "                                          subcharIdx  \n",
       "0  [[527, 971], [566, 948], [55, 55], [810, 1021,...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 440 ms, sys: 532 ms, total: 972 ms\n",
      "Wall time: 5.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = extract_features(dev)\n",
    "dev_feature = temp.drop(columns=[\"token\",\"token_indices\"])\n",
    "temp = extract_features(test)\n",
    "test_feature = temp.drop(columns=[\"token\",\"token_indices\"])\n",
    "# test_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sequence_num</th>\n",
       "      <th>label</th>\n",
       "      <th>bio</th>\n",
       "      <th>bio_only</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenList</th>\n",
       "      <th>tokenIdx</th>\n",
       "      <th>redIdx</th>\n",
       "      <th>subcharIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
       "      <td>我们变而以书会友，以书结缘，把欧美、港台流行的食品类图谱、画册、工具书汇集一堂。</td>\n",
       "      <td>[我, 们, 变, 而, 以, 书, 会, 友, ，, 以, 书, 结, 缘, ，, 把, ...</td>\n",
       "      <td>[4205, 10, 2053, 3285, 3856, 2545, 2936, 3958,...</td>\n",
       "      <td>[162, 220, 225, 33, 139, 86, 139, 225, 194, 13...</td>\n",
       "      <td>[[1066, 190], [152, 236], [693, 544], [1071, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
       "      <td>为了跟踪国际最新食品工艺、流行趋势，大量搜集海外专业书刊资料是提高技艺的捷径。</td>\n",
       "      <td>[为, 了, 跟, 踪, 国, 际, 最, 新, 食, 品, 工, 艺, 、, 流, 行, ...</td>\n",
       "      <td>[2665, 1607, 3912, 2504, 994, 2033, 3484, 326,...</td>\n",
       "      <td>[203, 42, 9, 9, 132, 80, 25, 48, 183, 112, 24,...</td>\n",
       "      <td>[[1107, 846, 1027, 1107], [951, 926], [568, 98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
       "      <td>其中线装古籍逾千册；民国出版物几百种；珍本四册、稀见本四百余册，出版时间跨越三百余年。</td>\n",
       "      <td>[其, 中, 线, 装, 古, 籍, 逾, 千, 册, ；, 民, 国, 出, 版, 物, ...</td>\n",
       "      <td>[1503, 2373, 1721, 2301, 2258, 324, 1980, 2754...</td>\n",
       "      <td>[69, 40, 232, 70, 112, 138, 188, 89, 227, 194,...</td>\n",
       "      <td>[[246, 1071, 357], [977, 604], [771, 718], [88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
       "      <td>有的古木交柯，春机荣欣，从诗人句中得之，而入画中，观之令人心驰。</td>\n",
       "      <td>[有, 的, 古, 木, 交, 柯, ，, 春, 机, 荣, 欣, ，, 从, 诗, 人, ...</td>\n",
       "      <td>[2077, 2120, 2258, 3978, 861, 3957, 1137, 1310...</td>\n",
       "      <td>[95, 113, 112, 174, 176, 174, 194, 215, 174, 1...</td>\n",
       "      <td>[[1071, 846, 103], [123, 948], [954, 977], [95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
       "      <td>不过重在晋趣，略增明人气息，妙在集古有道、不露痕迹罢了。</td>\n",
       "      <td>[不, 过, 重, 在, 晋, 趣, ，, 略, 增, 明, 人, 气, 息, ，, 妙, ...</td>\n",
       "      <td>[751, 4085, 1386, 4064, 3780, 2109, 1137, 4199...</td>\n",
       "      <td>[170, 188, 233, 131, 215, 106, 194, 190, 131, ...</td>\n",
       "      <td>[[1071, 846, 604, 1107], [848, 564], [846, 107...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sequence_num  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                               label  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                                 bio  \\\n",
       "0  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...   \n",
       "1  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...   \n",
       "2  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...   \n",
       "3  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...   \n",
       "4  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...   \n",
       "\n",
       "                                            bio_only  \\\n",
       "0  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...   \n",
       "1  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...   \n",
       "2  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...   \n",
       "3  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...   \n",
       "4  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...   \n",
       "\n",
       "                                          raws  \\\n",
       "0     我们变而以书会友，以书结缘，把欧美、港台流行的食品类图谱、画册、工具书汇集一堂。   \n",
       "1      为了跟踪国际最新食品工艺、流行趋势，大量搜集海外专业书刊资料是提高技艺的捷径。   \n",
       "2  其中线装古籍逾千册；民国出版物几百种；珍本四册、稀见本四百余册，出版时间跨越三百余年。   \n",
       "3             有的古木交柯，春机荣欣，从诗人句中得之，而入画中，观之令人心驰。   \n",
       "4                 不过重在晋趣，略增明人气息，妙在集古有道、不露痕迹罢了。   \n",
       "\n",
       "                                           tokenList  \\\n",
       "0  [我, 们, 变, 而, 以, 书, 会, 友, ，, 以, 书, 结, 缘, ，, 把, ...   \n",
       "1  [为, 了, 跟, 踪, 国, 际, 最, 新, 食, 品, 工, 艺, 、, 流, 行, ...   \n",
       "2  [其, 中, 线, 装, 古, 籍, 逾, 千, 册, ；, 民, 国, 出, 版, 物, ...   \n",
       "3  [有, 的, 古, 木, 交, 柯, ，, 春, 机, 荣, 欣, ，, 从, 诗, 人, ...   \n",
       "4  [不, 过, 重, 在, 晋, 趣, ，, 略, 增, 明, 人, 气, 息, ，, 妙, ...   \n",
       "\n",
       "                                            tokenIdx  \\\n",
       "0  [4205, 10, 2053, 3285, 3856, 2545, 2936, 3958,...   \n",
       "1  [2665, 1607, 3912, 2504, 994, 2033, 3484, 326,...   \n",
       "2  [1503, 2373, 1721, 2301, 2258, 324, 1980, 2754...   \n",
       "3  [2077, 2120, 2258, 3978, 861, 3957, 1137, 1310...   \n",
       "4  [751, 4085, 1386, 4064, 3780, 2109, 1137, 4199...   \n",
       "\n",
       "                                              redIdx  \\\n",
       "0  [162, 220, 225, 33, 139, 86, 139, 225, 194, 13...   \n",
       "1  [203, 42, 9, 9, 132, 80, 25, 48, 183, 112, 24,...   \n",
       "2  [69, 40, 232, 70, 112, 138, 188, 89, 227, 194,...   \n",
       "3  [95, 113, 112, 174, 176, 174, 194, 215, 174, 1...   \n",
       "4  [170, 188, 233, 131, 215, 106, 194, 190, 131, ...   \n",
       "\n",
       "                                          subcharIdx  \n",
       "0  [[1066, 190], [152, 236], [693, 544], [1071, 1...  \n",
       "1  [[1107, 846, 1027, 1107], [951, 926], [568, 98...  \n",
       "2  [[246, 1071, 357], [977, 604], [771, 718], [88...  \n",
       "3  [[1071, 846, 103], [123, 948], [954, 977], [95...  \n",
       "4  [[1071, 846, 604, 1107], [848, 564], [846, 107...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574\n",
      "577\n",
      "568\n",
      "577\n"
     ]
    }
   ],
   "source": [
    "def find_longest_sequence(data_with_features):\n",
    "#     assert (np.max(list(map(len, data_with_features[\"tokenIdx\"])))) == (np.max(list(map(len, data_with_features[\"bmes\"]))))\n",
    "    return (np.max(list(map(len, data_with_features[\"tokenIdx\"]))))\n",
    "\n",
    "train_longest = find_longest_sequence(train_feature)\n",
    "print(train_longest)\n",
    "test_longest = find_longest_sequence(test_feature)\n",
    "print(test_longest)\n",
    "dev_longest = find_longest_sequence(dev_feature)\n",
    "print(dev_longest)\n",
    "seq_longest = np.max([train_longest,test_longest,dev_longest])\n",
    "print(seq_longest)\n",
    "\n",
    "subchar_seq_length = np.max(list(map(lambda x : len(getSubChar(x)),train_dic_char)))\n",
    "subchar_seq_length = 3\n",
    "subsubchar_padtok = oov_subchar + 1\n",
    "seq_length = seq_longest\n",
    "\n",
    "def padd_char(seq):\n",
    "    temp_char_seqs_padded = []\n",
    "    for item in seq[\"subcharIdx\"]:\n",
    "        temp_pad = pad_sequences(item, maxlen=subchar_seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=subsubchar_padtok)\n",
    "  \n",
    "        a = temp_pad\n",
    "        b = [[subsubchar_padtok for i in range(subchar_seq_length)] for _ in range(0, seq_length - len(temp_pad))]\n",
    "        if len(b) == 0:\n",
    "            c = a\n",
    "        else:\n",
    "            c = np.concatenate((a, b))\n",
    "        \n",
    "    # print(len(c))\n",
    "        temp_char_seqs_padded.append(c)\n",
    "  # print(len(temp_char_seqs_padded))\n",
    "    return temp_char_seqs_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subchar_seqs_padded = padd_char(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padding token index is 4313\n"
     ]
    }
   ],
   "source": [
    "seq_length = seq_longest\n",
    "\n",
    "# a new dummy token index, one more than OOV\n",
    "padtok = oov+1\n",
    "red_padtok = oov_redical + 1\n",
    "print('The padding token index is %i' % padtok)\n",
    "\n",
    "# use pad_sequences, padding or truncating at the end of the sequence (default is 'pre')\n",
    "train_seqs_padded = pad_sequences(train_feature['tokenIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
    "# print('Example of padded token sequence:')\n",
    "# print(train_seqs_padded[1])\n",
    "\n",
    "\n",
    "train_red_padded = pad_sequences(train_feature['redIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=red_padtok)\n",
    "\n",
    "\n",
    "train_subchar_seqs_padded = padd_char(train_feature)\n",
    "\n",
    "\n",
    "# Prepare Dev set.\n",
    "\n",
    "dev_seqs_padded = pad_sequences(dev_feature['tokenIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
    "\n",
    "dev_red_padded = pad_sequences(dev_feature['redIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=red_padtok)\n",
    "\n",
    "dev_subchar_seqs_padded = padd_char(dev_feature)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare Test set.\n",
    "test_seqs_padded = pad_sequences(test_feature['tokenIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
    "\n",
    "test_rad_padded = pad_sequences(test_feature['redIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=red_padtok)\n",
    "\n",
    "test_subchar_seqs_padded = padd_char(test_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get lists of named entity labels, padded with a null label (=3)\n",
    "# # Full BIOs\n",
    "\n",
    "\n",
    "# padlab = 7\n",
    "# n_labs = 8\n",
    "\n",
    "# train_labs_padded = pad_sequences(train_feature['bio'].tolist(), maxlen=seq_length,\n",
    "#                                   dtype='int32', padding='post', truncating='post', value=padlab)\n",
    "\n",
    "# # convert those labels to one-hot encoding\n",
    "# train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]\n",
    "\n",
    "# # # follow the print outputs below to see how the labels are transformed\n",
    "# # print('Length of input sequence: %i' % len(train_labs_padded[1]))\n",
    "# # print('Length of label sequence: %i' % len(train_labs_onehot[1]))\n",
    "# # print(train_labs_padded[1][:11])\n",
    "# # print(train_labs_onehot[1][:11])\n",
    "\n",
    "\n",
    "\n",
    "# dev_labs_padded = pad_sequences(dev_feature['bio'].tolist(), maxlen=seq_length,\n",
    "#                                   dtype='int32', padding='post', truncating='post', value=padlab)\n",
    "\n",
    "# dev_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in dev_labs_padded]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lists of named entity labels, padded with a null label (=3)\n",
    "# Full BIOs\n",
    "\n",
    "\n",
    "padlab = 3\n",
    "n_labs = 4\n",
    "\n",
    "train_labs_padded = pad_sequences(train_feature['bio_only'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padlab)\n",
    "\n",
    "# convert those labels to one-hot encoding\n",
    "train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]\n",
    "\n",
    "# # follow the print outputs below to see how the labels are transformed\n",
    "# print('Length of input sequence: %i' % len(train_labs_padded[1]))\n",
    "# print('Length of label sequence: %i' % len(train_labs_onehot[1]))\n",
    "# print(train_labs_padded[1][:11])\n",
    "# print(train_labs_onehot[1][:11])\n",
    "\n",
    "\n",
    "\n",
    "dev_labs_padded = pad_sequences(dev_feature['bio_only'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padlab)\n",
    "\n",
    "dev_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in dev_labs_padded]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use deep copy to ensure we aren't updating original values\n",
    "# import copy\n",
    "# train_weights_onehot = copy.deepcopy(train_labs_onehot)\n",
    "\n",
    "# # our first-pass class weights: normal for named entities (0 and 1), down-weighted for non named entities (2 and 3)\n",
    "# class_wts = [1,1,.1,.1]\n",
    "\n",
    "# # apply our weights to the label lists\n",
    "# for i,labs in enumerate(train_weights_onehot):\n",
    "#     for j,lablist in enumerate(labs):\n",
    "#         lablistaslist = lablist.tolist()\n",
    "#         whichismax = lablistaslist.index(max(lablistaslist))\n",
    "#         train_weights_onehot[i][j][whichismax] = class_wts[whichismax]\n",
    "        \n",
    "        \n",
    "# dev_weights_onehot = copy.deepcopy(dev_labs_onehot)\n",
    "\n",
    "# # our first-pass class weights: normal for named entities (0 and 1), down-weighted for non named entities (2 and 3)\n",
    "# class_wts = [1,1,.1,.1]\n",
    "\n",
    "# # apply our weights to the label lists\n",
    "# for i,labs in enumerate(dev_weights_onehot):\n",
    "#     for j,lablist in enumerate(labs):\n",
    "#         lablistaslist = lablist.tolist()\n",
    "#         whichismax = lablistaslist.index(max(lablistaslist))\n",
    "#         dev_weights_onehot[i][j][whichismax] = class_wts[whichismax]\n",
    "\n",
    "# # # what's this like, before and after?\n",
    "# # print('Initial one-hot label encoding:')\n",
    "# # print(train_labs_onehot[1][:11])\n",
    "\n",
    "# # print('Weighted label encoding:')\n",
    "# # print(dev_weights_onehot[1][:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "**Defining a neural network**\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "tok_input1 (InputLayer)         [(None, 577)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "red_input2 (InputLayer)         [(None, 577)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 577, 128)     552192      tok_input1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 577, 128)     30336       red_input2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 577, 256)     0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 577, 100)     122800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 577, 100)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "char_input3 (InputLayer)        [(None, 577, 3)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 577, 4)       404         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 705,732\n",
      "Trainable params: 705,732\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load Keras and TensorFlow\n",
    "\n",
    "\n",
    "\n",
    "# our final vocab size is the padding token + 1 (OR length of vocab + OOV + PAD)\n",
    "vocab_size = padtok+1\n",
    "red_size = red_padtok+1\n",
    "subchar_size = subsubchar_padtok + 1\n",
    "\n",
    "print(vocab_size==len(train_dic_char)+2)\n",
    "embed_size = 128 # y an embedding size of 128 (could tune this)\n",
    "\n",
    "# list of metrics to use: true & false positives, negatives, accuracy, precision, recall, area under the curve\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "# our model has the option for an label prediction bias, it's sequential, starts with an embedding layer, then bi-LSTM,\n",
    "# a dropout layer follows for regularisation, and a dense final layer with softmax activation to output class probabilities\n",
    "# we compile with the Adam optimizer at a low learning rate, use categorical cross-entropy as our loss function\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:        \n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    tok_input1 = keras.layers.Input(shape=(seq_length,), dtype='int32', name='tok_input1')\n",
    "    red_input2 = keras.layers.Input(shape=(seq_length,), dtype='int32', name='red_input2')\n",
    "    subchar_input3 = keras.layers.Input(shape=(seq_length,subchar_seq_length), dtype='int32', name='char_input3')\n",
    "    emb_char = keras.layers.TimeDistributed(keras.layers.Embedding(output_dim=embed_size, input_dim=subchar_size, input_length=3,  mask_zero=True, trainable=True))(subchar_input3)\n",
    "    char_enc = keras.layers.TimeDistributed(keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=False, dropout=0.2,recurrent_dropout=0.2)))(emb_char)\n",
    "    x1 = keras.layers.Embedding(output_dim=embed_size, input_dim=vocab_size,  input_length=seq_length,  mask_zero=True, trainable=True)(tok_input1)\n",
    "    x2 = keras.layers.Embedding(output_dim=embed_size, input_dim=red_size,  input_length=seq_length, mask_zero=True, trainable=True)(red_input2)\n",
    "    x_cancat = keras.layers.concatenate([x1,x2])\n",
    "#     x_cancat = (x1)\n",
    "    x_lstm = keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2))(x_cancat)\n",
    "    x_drop = keras.layers.Dropout(0.5)(x_lstm)\n",
    "    main_output = keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias))(x_drop)\n",
    "    model = keras.models.Model(inputs=[tok_input1,red_input2, subchar_input3], outputs= main_output)\n",
    "#     model = keras.models.Model(inputs=[tok_input1, pos_input2, char_input3], outputs= main_output)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Old lstm model\n",
    "#     model = keras.Sequential([\n",
    "#         keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True),\n",
    "#         keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2)),  # 2 directions, 50 units each, concatenated (can change this)\n",
    "#         keras.layers.Dropout(0.5),\n",
    "#         keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias)),\n",
    "#     ])\n",
    "\n",
    "\n",
    "# early stopping criteria based on area under the curve: will stop if no improvement after 10 epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n",
    "\n",
    "# the number of training epochs we'll use, and the batch size (how many texts are input at once)\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "print('**Defining a neural network**')\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # evaluate our initial model\n",
    "# results = model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=0)\n",
    "# print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 11059348, 2: 869087, 1: 76101, 0: 33992})\n",
      "12038528\n",
      "Initial bias:\n",
      "[0.002823601024975811, 0.006321453918618622, 0.0721921317955152, 0.9186628132608904]\n"
     ]
    }
   ],
   "source": [
    "# figure out the label distribution in our fixed-length texts\n",
    "from collections import Counter\n",
    "all_labs = [l for lab in train_labs_padded for l in lab]\n",
    "label_count = Counter(all_labs)\n",
    "total_labs = len(all_labs)\n",
    "print(label_count)\n",
    "print(total_labs)\n",
    "\n",
    "initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n",
    "              (label_count[2]/total_labs), (label_count[3]/total_labs)]\n",
    "\n",
    "# initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n",
    "#               (label_count[2]/total_labs), (label_count[3]/total_labs), (label_count[4]/total_labs),\n",
    "#               (label_count[5]/total_labs), (label_count[6]/total_labs), (label_count[7]/total_labs)]\n",
    "\n",
    "print('Initial bias:')\n",
    "print(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare sequences and labels as numpy arrays, check dimensions\n",
    "X = np.array(train_seqs_padded)\n",
    "X_red = np.array(train_red_padded)\n",
    "X_sub = np.array(train_subchar_seqs_padded)\n",
    "y = np.array(train_labs_onehot)\n",
    "# y = np.array(train_weights_onehot)\n",
    "\n",
    "\n",
    "# print('Input sequence dimensions (n.docs, seq.length):')\n",
    "# print(X.shape)\n",
    "# print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = np.array(dev_seqs_padded)\n",
    "X_red_dev = np.array(dev_red_padded)\n",
    "X_sub_dev = np.array(dev_subchar_seqs_padded)\n",
    "y_dev = np.array(dev_labs_onehot)\n",
    "# y_dev = np.array(dev_weights_onehot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 67s 409ms/step - loss: 0.0785 - tp: 11626015.0000 - fp: 110287.0000 - tn: 36005296.0000 - fn: 412513.0000 - accuracy: 0.9891 - precision: 0.9906 - recall: 0.9657 - auc: 0.9991 - val_loss: 0.0216 - val_tp: 1324625.0000 - val_fp: 7233.0000 - val_tn: 4005225.0000 - val_fn: 12861.0000 - val_accuracy: 0.9962 - val_precision: 0.9946 - val_recall: 0.9904 - val_auc: 0.9999\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 66s 402ms/step - loss: 0.0186 - tp: 11935804.0000 - fp: 55561.0000 - tn: 36060020.0000 - fn: 102724.0000 - accuracy: 0.9967 - precision: 0.9954 - recall: 0.9915 - auc: 0.9999 - val_loss: 0.0147 - val_tp: 1328264.0000 - val_fp: 5218.0000 - val_tn: 4007240.0000 - val_fn: 9222.0000 - val_accuracy: 0.9973 - val_precision: 0.9961 - val_recall: 0.9931 - val_auc: 0.9999\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 65s 401ms/step - loss: 0.0138 - tp: 11964269.0000 - fp: 45651.0000 - tn: 36069928.0000 - fn: 74259.0000 - accuracy: 0.9975 - precision: 0.9962 - recall: 0.9938 - auc: 0.9999 - val_loss: 0.0113 - val_tp: 1331005.0000 - val_fp: 4418.0000 - val_tn: 4008040.0000 - val_fn: 6481.0000 - val_accuracy: 0.9980 - val_precision: 0.9967 - val_recall: 0.9952 - val_auc: 0.9999\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 65s 402ms/step - loss: 0.0112 - tp: 11981200.0000 - fp: 39797.0000 - tn: 36075800.0000 - fn: 57328.0000 - accuracy: 0.9980 - precision: 0.9967 - recall: 0.9952 - auc: 0.9999 - val_loss: 0.0097 - val_tp: 1332007.0000 - val_fp: 3985.0000 - val_tn: 4008473.0000 - val_fn: 5479.0000 - val_accuracy: 0.9982 - val_precision: 0.9970 - val_recall: 0.9959 - val_auc: 0.9999\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 65s 401ms/step - loss: 0.0097 - tp: 11989181.0000 - fp: 35665.0000 - tn: 36079912.0000 - fn: 49347.0000 - accuracy: 0.9982 - precision: 0.9970 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0090 - val_tp: 1332518.0000 - val_fp: 3801.0000 - val_tn: 4008657.0000 - val_fn: 4968.0000 - val_accuracy: 0.9984 - val_precision: 0.9972 - val_recall: 0.9963 - val_auc: 0.9999\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 66s 402ms/step - loss: 0.0088 - tp: 11994068.0000 - fp: 33041.0000 - tn: 36082532.0000 - fn: 44460.0000 - accuracy: 0.9984 - precision: 0.9973 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0082 - val_tp: 1332877.0000 - val_fp: 3544.0000 - val_tn: 4008914.0000 - val_fn: 4609.0000 - val_accuracy: 0.9985 - val_precision: 0.9973 - val_recall: 0.9966 - val_auc: 0.9999\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 65s 402ms/step - loss: 0.0081 - tp: 11997735.0000 - fp: 30924.0000 - tn: 36084656.0000 - fn: 40793.0000 - accuracy: 0.9985 - precision: 0.9974 - recall: 0.9966 - auc: 0.9999 - val_loss: 0.0079 - val_tp: 1333106.0000 - val_fp: 3419.0000 - val_tn: 4009039.0000 - val_fn: 4380.0000 - val_accuracy: 0.9985 - val_precision: 0.9974 - val_recall: 0.9967 - val_auc: 0.9999\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 66s 402ms/step - loss: 0.0076 - tp: 12000191.0000 - fp: 29135.0000 - tn: 36086448.0000 - fn: 38337.0000 - accuracy: 0.9986 - precision: 0.9976 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0075 - val_tp: 1333298.0000 - val_fp: 3320.0000 - val_tn: 4009138.0000 - val_fn: 4188.0000 - val_accuracy: 0.9986 - val_precision: 0.9975 - val_recall: 0.9969 - val_auc: 0.9999\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 66s 403ms/step - loss: 0.0072 - tp: 12002386.0000 - fp: 27609.0000 - tn: 36087968.0000 - fn: 36142.0000 - accuracy: 0.9987 - precision: 0.9977 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0072 - val_tp: 1333420.0000 - val_fp: 3228.0000 - val_tn: 4009230.0000 - val_fn: 4066.0000 - val_accuracy: 0.9986 - val_precision: 0.9976 - val_recall: 0.9970 - val_auc: 0.9999\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 66s 404ms/step - loss: 0.0067 - tp: 12004451.0000 - fp: 26061.0000 - tn: 36089520.0000 - fn: 34077.0000 - accuracy: 0.9988 - precision: 0.9978 - recall: 0.9972 - auc: 0.9999 - val_loss: 0.0070 - val_tp: 1333585.0000 - val_fp: 3125.0000 - val_tn: 4009333.0000 - val_fn: 3901.0000 - val_accuracy: 0.9987 - val_precision: 0.9977 - val_recall: 0.9971 - val_auc: 0.9999\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 65s 402ms/step - loss: 0.0064 - tp: 12006147.0000 - fp: 24707.0000 - tn: 36090880.0000 - fn: 32381.0000 - accuracy: 0.9988 - precision: 0.9979 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0070 - val_tp: 1333678.0000 - val_fp: 3085.0000 - val_tn: 4009373.0000 - val_fn: 3808.0000 - val_accuracy: 0.9987 - val_precision: 0.9977 - val_recall: 0.9972 - val_auc: 0.9999\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 65s 402ms/step - loss: 0.0061 - tp: 12007232.0000 - fp: 23855.0000 - tn: 36091724.0000 - fn: 31296.0000 - accuracy: 0.9989 - precision: 0.9980 - recall: 0.9974 - auc: 1.0000 - val_loss: 0.0067 - val_tp: 1333789.0000 - val_fp: 2963.0000 - val_tn: 4009495.0000 - val_fn: 3697.0000 - val_accuracy: 0.9988 - val_precision: 0.9978 - val_recall: 0.9972 - val_auc: 0.9999\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 66s 404ms/step - loss: 0.0059 - tp: 12008505.0000 - fp: 22945.0000 - tn: 36092648.0000 - fn: 30023.0000 - accuracy: 0.9989 - precision: 0.9981 - recall: 0.9975 - auc: 1.0000 - val_loss: 0.0066 - val_tp: 1333817.0000 - val_fp: 2940.0000 - val_tn: 4009518.0000 - val_fn: 3669.0000 - val_accuracy: 0.9988 - val_precision: 0.9978 - val_recall: 0.9973 - val_auc: 0.9999\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 66s 403ms/step - loss: 0.0055 - tp: 12010201.0000 - fp: 21538.0000 - tn: 36094048.0000 - fn: 28327.0000 - accuracy: 0.9990 - precision: 0.9982 - recall: 0.9976 - auc: 1.0000 - val_loss: 0.0067 - val_tp: 1333924.0000 - val_fp: 2918.0000 - val_tn: 4009540.0000 - val_fn: 3562.0000 - val_accuracy: 0.9988 - val_precision: 0.9978 - val_recall: 0.9973 - val_auc: 0.9999\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 66s 402ms/step - loss: 0.0053 - tp: 12011054.0000 - fp: 20828.0000 - tn: 36094756.0000 - fn: 27474.0000 - accuracy: 0.9990 - precision: 0.9983 - recall: 0.9977 - auc: 1.0000 - val_loss: 0.0064 - val_tp: 1333990.0000 - val_fp: 2837.0000 - val_tn: 4009621.0000 - val_fn: 3496.0000 - val_accuracy: 0.9988 - val_precision: 0.9979 - val_recall: 0.9974 - val_auc: 0.9999\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 66s 402ms/step - loss: 0.0051 - tp: 12012384.0000 - fp: 19825.0000 - tn: 36095744.0000 - fn: 26144.0000 - accuracy: 0.9990 - precision: 0.9984 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.0064 - val_tp: 1334066.0000 - val_fp: 2770.0000 - val_tn: 4009688.0000 - val_fn: 3420.0000 - val_accuracy: 0.9988 - val_precision: 0.9979 - val_recall: 0.9974 - val_auc: 0.9999\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 66s 404ms/step - loss: 0.0049 - tp: 12013228.0000 - fp: 19029.0000 - tn: 36096552.0000 - fn: 25300.0000 - accuracy: 0.9991 - precision: 0.9984 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0064 - val_tp: 1334132.0000 - val_fp: 2733.0000 - val_tn: 4009725.0000 - val_fn: 3354.0000 - val_accuracy: 0.9989 - val_precision: 0.9980 - val_recall: 0.9975 - val_auc: 0.9999\n",
      "Epoch 18/50\n",
      " 66/163 [===========>..................] - ETA: 37s - loss: 0.0046 - tp: 4864820.0000 - fp: 7262.0000 - tn: 14616226.0000 - fn: 9676.0000 - accuracy: 0.9991 - precision: 0.9985 - recall: 0.9980 - auc: 1.0000"
     ]
    }
   ],
   "source": [
    "# re-initiate model with bias\n",
    "model = make_model(output_bias=initial_bias)\n",
    "# and fit...\n",
    "model.fit([X,X_red,X_sub], y, batch_size=128, epochs=50, callbacks = [early_stopping],   validation_data=([X_dev,X_red_dev,X_sub_dev], y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 2455784, 2: 187794, 1: 22252, 0: 9142})\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(test_seqs_padded)\n",
    "X_test_rad = np.array(test_rad_padded)\n",
    "X_test_subchar = np.array(test_subchar_seqs_padded)\n",
    "\n",
    "# preds = np.argmax(model.predict(X_test), axis=-1)\n",
    "preds = np.argmax(model.predict([X_test,X_test_rad,X_test_subchar]), axis=-1)\n",
    "flat_preds = [p for pred in preds for p in pred]\n",
    "print(Counter(flat_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4636"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hu/miniconda3/envs/dev/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sequence_num</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>token_indices</th>\n",
       "      <th>bio</th>\n",
       "      <th>bio_only</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenList</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[我, 们, 变, 而, 以, 书, 会, 友, ，, 以, 书, 结, 缘, ，, 把, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[102.0, 408.0, 138.0, 44.0, 224.0, 517.0, 122....</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...</td>\n",
       "      <td>我们变而以书会友，以书结缘，把欧美、港台流行的食品类图谱、画册、工具书汇集一堂。</td>\n",
       "      <td>[我, 们, 变, 而, 以, 书, 会, 友, ，, 以, 书, 结, 缘, ，, 把, ...</td>\n",
       "      <td>[2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sequence_num  \\\n",
       "0           0             0   \n",
       "\n",
       "                                               token  \\\n",
       "0  [我, 们, 变, 而, 以, 书, 会, 友, ，, 以, 书, 结, 缘, ，, 把, ...   \n",
       "\n",
       "                                               label  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                       token_indices  \\\n",
       "0  [102.0, 408.0, 138.0, 44.0, 224.0, 517.0, 122....   \n",
       "\n",
       "                                                 bio  \\\n",
       "0  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, ...   \n",
       "\n",
       "                                            bio_only  \\\n",
       "0  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, ...   \n",
       "\n",
       "                                       raws  \\\n",
       "0  我们变而以书会友，以书结缘，把欧美、港台流行的食品类图谱、画册、工具书汇集一堂。   \n",
       "\n",
       "                                           tokenList  \\\n",
       "0  [我, 们, 变, 而, 以, 书, 会, 友, ，, 以, 书, 结, 缘, ，, 把, ...   \n",
       "\n",
       "                                          prediction  \n",
       "0  [2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seqs_temp['prediction'] = ''\n",
    "for i in test_seqs_temp.index:\n",
    "    this_seq_length = len(test_seqs_temp['tokenList'][i])\n",
    "    test_seqs_temp['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
    "# test_seqs_temp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_num</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>token_indices</th>\n",
       "      <th>bio</th>\n",
       "      <th>bio_only</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenList</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>我</td>\n",
       "      <td>O</td>\n",
       "      <td>102</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>我们变而以书会友，以书结缘，把欧美、港台流行的食品类图谱、画册、工具书汇集一堂。</td>\n",
       "      <td>我</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence_num  Unnamed: 0 token label token_indices bio bio_only  \\\n",
       "0             0           0     我     O           102   6        2   \n",
       "\n",
       "                                       raws tokenList prediction  \n",
       "0  我们变而以书会友，以书结缘，把欧美、港台流行的食品类图谱、画册、工具书汇集一堂。         我          2  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_long = test_seqs_temp.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
    "# test_long.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O    187803\n",
       "I     22252\n",
       "B      9142\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-using the BIO integer-to-character function from last time\n",
    "def reverse_bio(ind):\n",
    "  bio = 'O'  # for any pad=3 predictions\n",
    "  if ind==0:\n",
    "    bio = 'B'\n",
    "  elif ind==1:\n",
    "    bio = 'I'\n",
    "  elif ind==2:\n",
    "    bio = 'O'\n",
    "  return bio\n",
    "\n",
    "bio_labs = [reverse_bio(b) for b in test_long['bio_only']]\n",
    "test_long['bio_only'] = bio_labs\n",
    "pred_labs = [reverse_bio(b) for b in test_long['prediction']]\n",
    "test_long['prediction'] = pred_labs\n",
    "\n",
    "test_long.head()\n",
    "test_long.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of TP and FP = 8652\n",
      "Sum of TP and FN = 6167\n",
      "True positives = 5314, False positives = 3338, False negatives = 853\n",
      "Precision = 0.614, Recall = 0.862, F1 = 0.717 (max=1)\n"
     ]
    }
   ],
   "source": [
    "def wnut_evaluate(txt):\n",
    "  '''row by row entity evaluation: we evaluate by whole named entities'''\n",
    "  tp = 0; fp = 0; fn = 0\n",
    "  in_entity = 0\n",
    "  for i in txt.index:\n",
    "    if txt['prediction'][i]=='B' and txt['bio_only'][i]=='B':\n",
    "      if in_entity==1:  # if there's a preceding named entity which didn't have intervening O...\n",
    "        tp += 1  # count a true positive\n",
    "      in_entity = 1  # start tracking this entity (don't count it until we know full span of entity)\n",
    "    elif txt['prediction'][i]=='B':\n",
    "      fp += 1  # if not a B in gold annotations, it's a false positive\n",
    "      in_entity = 0\n",
    "    elif txt['prediction'][i]=='I' and txt['bio_only'][i]=='I':\n",
    "      next  # correct entity continuation: do nothing\n",
    "    elif txt['prediction'][i]=='I' and txt['bio_only'][i]=='B':\n",
    "      fn += 1  # if a new entity should have begun, it's a false negative\n",
    "      in_entity = 0\n",
    "    elif txt['prediction'][i]=='I':  # if gold is O...\n",
    "      if in_entity==1:  # and if tracking an entity, then the span is too long\n",
    "        fp += 1  # it's a false positive\n",
    "      in_entity = 0\n",
    "    elif txt['prediction'][i]=='O':\n",
    "      if txt['bio_only'][i]=='B':\n",
    "        fn += 1  # false negative if there's B in gold but no predicted B\n",
    "        if in_entity==1:  # also check if there was a named entity in progress\n",
    "          tp += 1  # count a true positive\n",
    "      elif txt['bio_only'][i]=='I':\n",
    "        if in_entity==1:  # if this should have been a continued named entity, the span is too short\n",
    "          fn += 1  # count a false negative\n",
    "      elif txt['bio_only'][i]=='O':\n",
    "        if in_entity==1:  # if a named entity has ended in right place\n",
    "          tp += 1  # count a true positive\n",
    "      in_entity = 0\n",
    "\n",
    "  if in_entity==1:  # catch any final named entity\n",
    "    tp += 1\n",
    "\n",
    "  print('Sum of TP and FP = %i' % (tp+fp))\n",
    "  print('Sum of TP and FN = %i' % (tp+fn))\n",
    "  print('True positives = %i, False positives = %i, False negatives = %i' % (tp, fp, fn))\n",
    "  prec = tp / (tp+fp)\n",
    "  rec = tp / (tp+fn)\n",
    "  f1 = (2*(prec*rec)) / (prec+rec)\n",
    "  print('Precision = %.3f, Recall = %.3f, F1 = %.3f (max=1)' % (prec, rec, f1))\n",
    " \n",
    "wnut_evaluate(test_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_long.to_csv('lstm21epocweighted1hot.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm21epoc\n",
    "# Sum of TP and FP = 6239\n",
    "# Sum of TP and FN = 6921\n",
    "# True positives = 4895, False positives = 1344, False negatives = 2026\n",
    "# Precision = 0.785, Recall = 0.707, F1 = 0.744 (max=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def postEditPred(pred):\n",
    "#     print(pred)\n",
    "#     ans = []\n",
    "#     for item in pred:\n",
    "#         if item == 0 or item == 3:\n",
    "#             ans.append(1)\n",
    "#         else:\n",
    "#             ans.append(0)\n",
    "            \n",
    "#     assert len(ans) == len(pred)\n",
    "#     return ans\n",
    "\n",
    "# def splitSentence(st, pred):\n",
    "#     temp_st = st\n",
    "#     temp_pred = pred\n",
    "#     temp_pred[0] = 2\n",
    "#     buf = []\n",
    "#     result = []\n",
    "#     for (pre,char) in zip(temp_pred,temp_st):\n",
    "#         if(pre == 0 or pre == 3):\n",
    "#             result.append(buf)\n",
    "#             buf = []\n",
    "#         buf.append(char)\n",
    "#     if(len(buf) > 0):\n",
    "#         result.append(buf)\n",
    "#     return result\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# def sentencePrediction(dataset,prediction):\n",
    "#     data_temp = dataset.copy()\n",
    "#     assert len(data_temp) == len(prediction)\n",
    "#     raw_sents = data_temp[\"raws\"]\n",
    "#     assert len(raw_sents) == len(prediction)\n",
    "#     ans = []\n",
    "#     for (st,pred) in zip(raw_sents,prediction):\n",
    "#         sptSt = splitSentence(st,pred)\n",
    "#         ans.append(sptSt)\n",
    "#     assert len(ans) == len(data_temp)\n",
    "#     data_temp[\"tokenList\"]= ans\n",
    "#     return data_temp\n",
    "    \n",
    "# def convertToPredSts(dataset):\n",
    "#     data_temp = dataset.copy()\n",
    "#     tokenList = data_temp[\"tokenList\"]\n",
    "#     ans = []\n",
    "    \n",
    "#     for item in tokenList:\n",
    "#         temp_st = list(map(lambda x : \"\".join(x), item))\n",
    "#         temp = \"  \".join(temp_st)\n",
    "#         temp = temp + \"  \"\n",
    "#         ans.append(temp)\n",
    "#     return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pred = sentencePrediction(test_feature,preds)\n",
    "output_sts = convertToPredSts(result_pred)\n",
    "\n",
    "f = open(\"./ans.txt\", \"w\")\n",
    "f.write(\"\\n\".join(output_sts))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(googlePath + \"./this.txt\", \"w\")\n",
    "f.write(\"\\n\".join(output_sts))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
