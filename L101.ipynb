{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the google colab\n",
    "!pip install xmnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# googlePath = \"./drive/MyDrive/L101Project/\"\n",
    "googlePath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import Session\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = Session(config=config)\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lazy Load) Loading model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['艹', '丿', '日', '王']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xmnlp\n",
    "xmnlp.radical('花丸晴琉')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up Chinese sub-character unites diactionary.\n",
    "chaiZi_Dic = {}\n",
    "with open(googlePath + \"corpus/chaizi/chaizi-jt.txt\", 'r') as f:\n",
    "    reader = csv.reader(f,delimiter='\\t')\n",
    "    for row in reader:\n",
    "        chaiZi_Dic[row[0]] = row[1]\n",
    "        \n",
    "def getSubChar(charCN):\n",
    "    if(charCN in chaiZi_Dic.keys()):\n",
    "        return chaiZi_Dic[item]\n",
    "    else:\n",
    "        return [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14563\n",
      "草 化\n",
      "['艹']\n",
      "\n",
      "丶 九\n",
      "['丿']\n",
      "\n",
      "[None]\n",
      "[None]\n",
      "\n",
      "日 青\n",
      "['日']\n",
      "\n",
      "玉 亠 厶 川\n",
      "['王']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(chaiZi_Dic.keys()))\n",
    "for item in \"花丸A晴琉\":\n",
    "    print(getSubChar(item))\n",
    "    print(xmnlp.radical(item))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkuTrain = googlePath + \"./corpus/cws/icwb2-data/training/pku_training.utf8\"\n",
    "pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "\n",
    "f = open(pkuTest, \"r\")\n",
    "temp = f.readlines()\n",
    "con = 0\n",
    "for item in temp:\n",
    "    con+=1\n",
    "    if(item == \"\\n\"):\n",
    "        print(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Corpus for Chinese Word Segamentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input\n",
       "0  迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pkuTrain = googlePath + \"./corpus/cws/icwb2-data/training/pku_training.utf8\"\n",
    "pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "train = pd.read_table(pkuTrain,  encoding='utf8', header=None, names=['input'])  # don't drop the empty lines yet, they show up as NaN in the data frame\n",
    "train.head(n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>raws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n",
       "      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   input                   raws\n",
       "0  共同创造美好的新世纪——二○○一年新年贺词  共同创造美好的新世纪——二○○一年新年贺词"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkuTest = googlePath  + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "test = pd.read_table(pkuTest,  encoding='utf8', header=None, names=['input']) \n",
    "test[\"raws\"] = test[\"input\"]\n",
    "test.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19054\n"
     ]
    }
   ],
   "source": [
    "def prepross_data(train_Data):\n",
    "    inputs = train_Data[\"input\"][:]\n",
    "    print(len(inputs))\n",
    "    raws = []\n",
    "    tokenList = []\n",
    "    \n",
    "    for item in inputs:\n",
    "        item = item.replace(\"   \", \"  \")\n",
    "\n",
    "        raw = item.replace(\" \", \"\")\n",
    "#         print(item)\n",
    "#         print(raw)\n",
    "        raws.append(raw)\n",
    "        tokens = (item.split(\"  \")[:-1])\n",
    "        tokenList.append(tokens)\n",
    "#         print(tokens)\n",
    "    train_Data[\"raws\"] = raws\n",
    "    train_Data[\"tokenList\"] = tokenList\n",
    "    dictionary_train_word = list(set(reduce(operator.add,train_Data[\"tokenList\"])))\n",
    "    dictionary_train_char =  list(set(reduce(operator.add,train_Data[\"raws\"])))    \n",
    "    return train_Data, dictionary_train_char, dictionary_train_word\n",
    "train, train_dic_char, train_dic_word  = prepross_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the pre-processing\n",
    "def validate_data(data):\n",
    "    tkList = data[\"tokenList\"]\n",
    "    raws = data[\"raws\"]\n",
    "    for (tokens, raw) in zip(tkList,raws):\n",
    "#         print(tokens)\n",
    "        \n",
    "#         print(raw)\n",
    "        temp  = \"\".join(tokens)\n",
    "#         print(temp)\n",
    "        assert(temp == raw)\n",
    "        if not (temp == raw):\n",
    "            print(temp)\n",
    "            print(raw)\n",
    "            print(tokens)\n",
    "validate_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...</td>\n",
       "      <td>迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）</td>\n",
       "      <td>[迈向, 充满, 希望, 的, 新, 世纪, ——, 一九九八年, 新年, 讲话, （, 附...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中共中央  总书记  、  国家  主席  江  泽民</td>\n",
       "      <td>中共中央总书记、国家主席江泽民</td>\n",
       "      <td>[中共中央, 总书记, 、, 国家, 主席, 江, 泽民]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>（  一九九七年  十二月  三十一日  ）</td>\n",
       "      <td>（一九九七年十二月三十一日）</td>\n",
       "      <td>[（, 一九九七年, 十二月, 三十一日, ）]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>１２月  ３１日  ，  中共中央  总书记  、  国家  主席  江  泽民  发表  ...</td>\n",
       "      <td>１２月３１日，中共中央总书记、国家主席江泽民发表１９９８年新年讲话《迈向充满希望的新世纪》。...</td>\n",
       "      <td>[１２月, ３１日, ，, 中共中央, 总书记, 、, 国家, 主席, 江, 泽民, 发表,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>同胞  们  、  朋友  们  、  女士  们  、  先生  们  ：</td>\n",
       "      <td>同胞们、朋友们、女士们、先生们：</td>\n",
       "      <td>[同胞, 们, 、, 朋友, 们, 、, 女士, 们, 、, 先生, 们, ：]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...   \n",
       "1                      中共中央  总书记  、  国家  主席  江  泽民     \n",
       "2                           （  一九九七年  十二月  三十一日  ）     \n",
       "3  １２月  ３１日  ，  中共中央  总书记  、  国家  主席  江  泽民  发表  ...   \n",
       "4           同胞  们  、  朋友  们  、  女士  们  、  先生  们  ：     \n",
       "\n",
       "                                                raws  \\\n",
       "0                       迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）   \n",
       "1                                    中共中央总书记、国家主席江泽民   \n",
       "2                                     （一九九七年十二月三十一日）   \n",
       "3  １２月３１日，中共中央总书记、国家主席江泽民发表１９９８年新年讲话《迈向充满希望的新世纪》。...   \n",
       "4                                   同胞们、朋友们、女士们、先生们：   \n",
       "\n",
       "                                           tokenList  \n",
       "0  [迈向, 充满, 希望, 的, 新, 世纪, ——, 一九九八年, 新年, 讲话, （, 附...  \n",
       "1                      [中共中央, 总书记, 、, 国家, 主席, 江, 泽民]  \n",
       "2                           [（, 一九九七年, 十二月, 三十一日, ）]  \n",
       "3  [１２月, ３１日, ，, 中共中央, 总书记, 、, 国家, 主席, 江, 泽民, 发表,...  \n",
       "4           [同胞, 们, 、, 朋友, 们, 、, 女士, 们, 、, 先生, 们, ：]  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4698\n",
      "4698\n",
      "55303\n",
      "55303\n",
      "55303\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dic_char))\n",
    "print(len(set(train_dic_char)))\n",
    "print(len(train_dic_word))\n",
    "print(len(set(train_dic_word)))\n",
    "with open(googlePath + \"./corpus/cws/icwb2-data/gold/pku_training_words.utf8\", 'r') as f:\n",
    "    content = f.readlines()  \n",
    "    print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4698\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "\n",
    "oov = len(train_dic_char)\n",
    "print(oov)\n",
    "def token_index(tok):\n",
    "    ind = tok\n",
    "    if tok in train_dic_char:  # if token in vocabulary\n",
    "        ind = train_dic_char.index(tok)\n",
    "    else:  # else it's OOV\n",
    "        ind = oov\n",
    "    return ind\n",
    "\n",
    "def str_token_index(string):\n",
    "    return [token_index(x)   for x in string]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training labels: convert BIO to integers\n",
    "# def bio_index(bio):\n",
    "#     ind = bio\n",
    "#     if not pd.isnull(bio):  # deal with empty lines\n",
    "#         if bio=='B':\n",
    "#             ind = 0\n",
    "#         elif bio=='I':\n",
    "#             ind = 1\n",
    "#         elif bio=='O':\n",
    "#             ind = 2\n",
    "#     return ind\n",
    "\n",
    "# Get Begin Middle End Single sequence\n",
    "# B 0 M 1 E 2 S 3\n",
    "\n",
    "def str_bmes_idx(tokenList):\n",
    "    answer = []\n",
    "    for item in tokenList:\n",
    "        if len(item) == 0:\n",
    "            raise NameErro(\"Zero Length Word\")\n",
    "        if len(item) == 1:\n",
    "            answer.append(3)\n",
    "        else:\n",
    "            answer.append(0)\n",
    "            for item in range(len(item) - 2):\n",
    "                answer.append(1)\n",
    "            answer.append(2)\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(data_set, isTest=False):\n",
    "    data_temp = data_set.copy()\n",
    "    \n",
    "\n",
    "    # Idx for chars\n",
    "    with Pool(8) as p:\n",
    "        tokinds = p.map(str_token_index,data_temp['raws'])\n",
    "#     tokinds = [list(map(token_index, u)) for u in data_temp['raws']]\n",
    "    data_temp[\"tokenIdx\"] = tokinds\n",
    "    \n",
    "    # BIO\n",
    "    if(not isTest):\n",
    "        data_temp[\"bmes\"] = [str_bmes_idx(u) for u in data_temp['tokenList']]\n",
    "        assert (list(map(len,data_temp[\"bmes\"])) == list(map(len,data_temp[\"tokenIdx\"])))\n",
    "\n",
    "        \n",
    "#     print(data_temp[\"bmes\"])\n",
    "#    assert reduce(operator.and_, list(map(len,data_temp[\"bmes\"])) == list(map(len,data_temp[\"tokenIdx\"])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return data_temp\n",
    "#     txt['token_indices'] = tokinds\n",
    "#     if not istest:  # can't do this with the test set\n",
    "#         bioints = [bio_index(b) for b in txt['bio_only']]\n",
    "#         txt['bio_only'] = bioints\n",
    "#     return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 422 ms, sys: 308 ms, total: 730 ms\n",
      "Wall time: 15.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenList</th>\n",
       "      <th>tokenIdx</th>\n",
       "      <th>bmes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...</td>\n",
       "      <td>迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）</td>\n",
       "      <td>[迈向, 充满, 希望, 的, 新, 世纪, ——, 一九九八年, 新年, 讲话, （, 附...</td>\n",
       "      <td>[2919, 4392, 4495, 726, 4078, 415, 3953, 186, ...</td>\n",
       "      <td>[0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 2, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中共中央  总书记  、  国家  主席  江  泽民</td>\n",
       "      <td>中共中央总书记、国家主席江泽民</td>\n",
       "      <td>[中共中央, 总书记, 、, 国家, 主席, 江, 泽民]</td>\n",
       "      <td>[3296, 1046, 3296, 1933, 640, 3190, 1018, 2806...</td>\n",
       "      <td>[0, 1, 1, 2, 0, 1, 2, 3, 0, 2, 0, 2, 3, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>（  一九九七年  十二月  三十一日  ）</td>\n",
       "      <td>（一九九七年十二月三十一日）</td>\n",
       "      <td>[（, 一九九七年, 十二月, 三十一日, ）]</td>\n",
       "      <td>[4488, 4274, 2877, 2877, 2356, 4089, 3229, 337...</td>\n",
       "      <td>[3, 0, 1, 1, 1, 2, 0, 1, 2, 0, 1, 1, 2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>１２月  ３１日  ，  中共中央  总书记  、  国家  主席  江  泽民  发表  ...</td>\n",
       "      <td>１２月３１日，中共中央总书记、国家主席江泽民发表１９９８年新年讲话《迈向充满希望的新世纪》。...</td>\n",
       "      <td>[１２月, ３１日, ，, 中共中央, 总书记, 、, 国家, 主席, 江, 泽民, 发表,...</td>\n",
       "      <td>[1897, 2078, 2209, 448, 1897, 2315, 1279, 3296...</td>\n",
       "      <td>[0, 1, 2, 0, 1, 2, 3, 0, 1, 1, 2, 0, 1, 2, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>同胞  们  、  朋友  们  、  女士  们  、  先生  们  ：</td>\n",
       "      <td>同胞们、朋友们、女士们、先生们：</td>\n",
       "      <td>[同胞, 们, 、, 朋友, 们, 、, 女士, 们, 、, 先生, 们, ：]</td>\n",
       "      <td>[858, 3372, 1236, 2806, 4371, 3772, 1236, 2806...</td>\n",
       "      <td>[0, 2, 3, 3, 0, 2, 3, 3, 0, 2, 3, 3, 0, 2, 3, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...   \n",
       "1                      中共中央  总书记  、  国家  主席  江  泽民     \n",
       "2                           （  一九九七年  十二月  三十一日  ）     \n",
       "3  １２月  ３１日  ，  中共中央  总书记  、  国家  主席  江  泽民  发表  ...   \n",
       "4           同胞  们  、  朋友  们  、  女士  们  、  先生  们  ：     \n",
       "\n",
       "                                                raws  \\\n",
       "0                       迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）   \n",
       "1                                    中共中央总书记、国家主席江泽民   \n",
       "2                                     （一九九七年十二月三十一日）   \n",
       "3  １２月３１日，中共中央总书记、国家主席江泽民发表１９９８年新年讲话《迈向充满希望的新世纪》。...   \n",
       "4                                   同胞们、朋友们、女士们、先生们：   \n",
       "\n",
       "                                           tokenList  \\\n",
       "0  [迈向, 充满, 希望, 的, 新, 世纪, ——, 一九九八年, 新年, 讲话, （, 附...   \n",
       "1                      [中共中央, 总书记, 、, 国家, 主席, 江, 泽民]   \n",
       "2                           [（, 一九九七年, 十二月, 三十一日, ）]   \n",
       "3  [１２月, ３１日, ，, 中共中央, 总书记, 、, 国家, 主席, 江, 泽民, 发表,...   \n",
       "4           [同胞, 们, 、, 朋友, 们, 、, 女士, 们, 、, 先生, 们, ：]   \n",
       "\n",
       "                                            tokenIdx  \\\n",
       "0  [2919, 4392, 4495, 726, 4078, 415, 3953, 186, ...   \n",
       "1  [3296, 1046, 3296, 1933, 640, 3190, 1018, 2806...   \n",
       "2  [4488, 4274, 2877, 2877, 2356, 4089, 3229, 337...   \n",
       "3  [1897, 2078, 2209, 448, 1897, 2315, 1279, 3296...   \n",
       "4  [858, 3372, 1236, 2806, 4371, 3772, 1236, 2806...   \n",
       "\n",
       "                                                bmes  \n",
       "0  [0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 2, 0, 1, 1, ...  \n",
       "1      [0, 1, 1, 2, 0, 1, 2, 3, 0, 2, 0, 2, 3, 0, 2]  \n",
       "2         [3, 0, 1, 1, 1, 2, 0, 1, 2, 0, 1, 1, 2, 3]  \n",
       "3  [0, 1, 2, 0, 1, 2, 3, 0, 1, 1, 2, 0, 1, 2, 3, ...  \n",
       "4   [0, 2, 3, 3, 0, 2, 3, 3, 0, 2, 3, 3, 0, 2, 3, 3]  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# MultiThreading\n",
    "train_feature = extract_features(train)\n",
    "train_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_sequence(data_with_features):\n",
    "#     assert (np.max(list(map(len, data_with_features[\"tokenIdx\"])))) == (np.max(list(map(len, data_with_features[\"bmes\"]))))\n",
    "    return (np.max(list(map(len, data_with_features[\"tokenIdx\"]))))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.2 ms, sys: 48.1 ms, total: 62.2 ms\n",
      "Wall time: 1.76 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n",
       "      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n",
       "      <td>[1046, 858, 37, 3118, 3328, 4588, 3953, 186, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>（二○○○年十二月三十一日）（附图片1张）</td>\n",
       "      <td>（二○○○年十二月三十一日）（附图片1张）</td>\n",
       "      <td>[4488, 3376, 2295, 2295, 2295, 4089, 3229, 337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女士们，先生们，同志们，朋友们：</td>\n",
       "      <td>女士们，先生们，同志们，朋友们：</td>\n",
       "      <td>[1896, 1480, 1236, 1279, 2547, 4104, 1236, 127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...</td>\n",
       "      <td>2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...</td>\n",
       "      <td>[1597, 2789, 2789, 1621, 4089, 186, 4089, 1728...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...</td>\n",
       "      <td>在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...</td>\n",
       "      <td>[3337, 1996, 1632, 3006, 1158, 2566, 817, 3953...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                              共同创造美好的新世纪——二○○一年新年贺词   \n",
       "1                              （二○○○年十二月三十一日）（附图片1张）   \n",
       "2                                   女士们，先生们，同志们，朋友们：   \n",
       "3  2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...   \n",
       "4  在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...   \n",
       "\n",
       "                                                raws  \\\n",
       "0                              共同创造美好的新世纪——二○○一年新年贺词   \n",
       "1                              （二○○○年十二月三十一日）（附图片1张）   \n",
       "2                                   女士们，先生们，同志们，朋友们：   \n",
       "3  2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...   \n",
       "4  在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...   \n",
       "\n",
       "                                            tokenIdx  \n",
       "0  [1046, 858, 37, 3118, 3328, 4588, 3953, 186, 3...  \n",
       "1  [4488, 3376, 2295, 2295, 2295, 4089, 3229, 337...  \n",
       "2  [1896, 1480, 1236, 1279, 2547, 4104, 1236, 127...  \n",
       "3  [1597, 2789, 2789, 1621, 4089, 186, 4089, 1728...  \n",
       "4  [3337, 1996, 1632, 3006, 1158, 2566, 817, 3953...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "temp = extract_features(test, isTest=True)\n",
    "test_feature = temp\n",
    "test_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n",
      "626\n",
      "1019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_longest = find_longest_sequence(train_feature)\n",
    "print(train_longest)\n",
    "\n",
    "test_longest = find_longest_sequence(test_feature)\n",
    "print(test_longest)\n",
    "\n",
    "seq_longest = np.max([train_longest,test_longest])\n",
    "print(seq_longest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padding token index is 4699\n",
      "Example of padded token sequence:\n",
      "[3296 1046 3296 ... 4699 4699 4699]\n"
     ]
    }
   ],
   "source": [
    "seq_length = seq_longest\n",
    "\n",
    "# a new dummy token index, one more than OOV\n",
    "padtok = oov+1\n",
    "print('The padding token index is %i' % padtok)\n",
    "\n",
    "# use pad_sequences, padding or truncating at the end of the sequence (default is 'pre')\n",
    "train_seqs_padded = pad_sequences(train_feature['tokenIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
    "print('Example of padded token sequence:')\n",
    "print(train_seqs_padded[1])\n",
    "\n",
    "# Prepare Test set.\n",
    "seq_length = seq_longest\n",
    "padtok = oov+1\n",
    "\n",
    "test_seqs_padded = pad_sequences(test_feature['tokenIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padtok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input sequence: 1019\n",
      "Length of label sequence: 1019\n",
      "[0 1 1 2 0 1 2 3 0 2 0]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# get lists of named entity labels, padded with a null label (=3)\n",
    "padlab = 4\n",
    "train_labs_padded = pad_sequences(train_feature['bmes'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padlab)\n",
    "\n",
    "# convert those labels to one-hot encoding\n",
    "n_labs = 5\n",
    "train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]\n",
    "\n",
    "# follow the print outputs below to see how the labels are transformed\n",
    "print('Length of input sequence: %i' % len(train_labs_padded[1]))\n",
    "print('Length of label sequence: %i' % len(train_labs_onehot[1]))\n",
    "print(train_labs_padded[1][:11])\n",
    "print(train_labs_onehot[1][:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence dimensions (n.docs, seq.length):\n",
      "(19054, 1019)\n",
      "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
      "(19054, 1019, 5)\n",
      "True\n",
      "**Defining a neural network**\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 1019, 128)         601600    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 1019, 100)         71600     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1019, 100)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 1019, 5)           505       \n",
      "=================================================================\n",
      "Total params: 673,705\n",
      "Trainable params: 673,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load Keras and TensorFlow\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# prepare sequences and labels as numpy arrays, check dimensions\n",
    "X = np.array(train_seqs_padded)\n",
    "y = np.array(train_labs_onehot)\n",
    "print('Input sequence dimensions (n.docs, seq.length):')\n",
    "print(X.shape)\n",
    "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
    "print(y.shape)\n",
    "\n",
    "# our final vocab size is the padding token + 1 (OR length of vocab + OOV + PAD)\n",
    "vocab_size = padtok+1\n",
    "print(vocab_size==len(train_dic_char)+2)\n",
    "embed_size = 128 # y an embedding size of 128 (could tune this)\n",
    "\n",
    "# list of metrics to use: true & false positives, negatives, accuracy, precision, recall, area under the curve\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "# our model has the option for an label prediction bias, it's sequential, starts with an embedding layer, then bi-LSTM,\n",
    "# a dropout layer follows for regularisation, and a dense final layer with softmax activation to output class probabilities\n",
    "# we compile with the Adam optimizer at a low learning rate, use categorical cross-entropy as our loss function\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2)),  # 2 directions, 50 units each, concatenated (can change this)\n",
    "#         keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),  # 2 directions, 50 units each, concatenated (can change this)\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias)),\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# early stopping criteria based on area under the curve: will stop if no improvement after 10 epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n",
    "\n",
    "# the number of training epochs we'll use, and the batch size (how many texts are input at once)\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "print('**Defining a neural network**')\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # evaluate our initial model\n",
    "# results = model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=0)\n",
    "# print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 17589578, 0: 585226, 2: 585226, 3: 524721, 1: 131275})\n",
      "19416026\n",
      "Initial bias:\n",
      "[0.03014138938627297, 0.006761167295511451, 0.03014138938627297, 0.027025149224666263, 0.9059309047072763]\n"
     ]
    }
   ],
   "source": [
    "# figure out the label distribution in our fixed-length texts\n",
    "from collections import Counter\n",
    "\n",
    "all_labs = [l for lab in train_labs_padded for l in lab]\n",
    "label_count = Counter(all_labs)\n",
    "total_labs = len(all_labs)\n",
    "print(label_count)\n",
    "print(total_labs)\n",
    "\n",
    "# use this to define an initial model bias\n",
    "initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n",
    "              (label_count[2]/total_labs), (label_count[3]/total_labs), (label_count[4]/total_labs)]\n",
    "print('Initial bias:')\n",
    "print(initial_bias)\n",
    "\n",
    "# # pass the bias to the model and re-evaluate\n",
    "# model = make_model(output_bias=initial_bias)\n",
    "# results = model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=0)\n",
    "# print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Train on 13337 samples, validate on 5717 samples\n",
      "Epoch 1/10\n",
      "13337/13337 [==============================] - 155s 12ms/sample - loss: 0.2262 - tp: 11499978.0000 - fp: 42462.0000 - tn: 54318732.0000 - fn: 2090320.0000 - accuracy: 0.9686 - precision: 0.9963 - recall: 0.8462 - auc: 0.9957 - val_loss: 0.1141 - val_tp: 5292816.0000 - val_fp: 1181.0000 - val_tn: 23301084.0000 - val_fn: 532751.0000 - val_accuracy: 0.9817 - val_precision: 0.9998 - val_recall: 0.9085 - val_auc: 0.9986\n",
      "Epoch 2/10\n",
      "13337/13337 [==============================] - 152s 11ms/sample - loss: 0.1027 - tp: 12593099.0000 - fp: 107597.0000 - tn: 54253596.0000 - fn: 997199.0000 - accuracy: 0.9837 - precision: 0.9915 - recall: 0.9266 - auc: 0.9986 - val_loss: 0.0769 - val_tp: 5557205.0000 - val_fp: 48897.0000 - val_tn: 23253372.0000 - val_fn: 268362.0000 - val_accuracy: 0.9891 - val_precision: 0.9913 - val_recall: 0.9539 - val_auc: 0.9993\n",
      "Epoch 3/10\n",
      "  512/13337 [>.............................] - ETA: 1:38 - loss: 0.0853 - tp: 496032.0000 - fp: 7693.0000 - tn: 2079191.0000 - fn: 25689.0000 - accuracy: 0.9872 - precision: 0.9847 - recall: 0.9508 - auc: 0.9991"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# re-initiate model with bias\n",
    "model = make_model(output_bias=initial_bias)\n",
    "\n",
    "# and fit...\n",
    "model.fit(X, y, batch_size=BATCH_SIZE, epochs=10, callbacks = [early_stopping],  validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 1809127, 2: 64042, 0: 59501, 3: 47181, 1: 1085})\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(test_seqs_padded)\n",
    "# preds = np.argmax(model.predict(X_test), axis=-1)\n",
    "preds = np.argmax(model.predict(X_test), axis=-1)\n",
    "flat_preds = [p for pred in preds for p in pred]\n",
    "print(Counter(flat_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1944"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def postEditPred(pred):\n",
    "    print(pred)\n",
    "    ans = []\n",
    "    for item in pred:\n",
    "        if item == 0 or item == 3:\n",
    "            ans.append(1)\n",
    "        else:\n",
    "            ans.append(0)\n",
    "            \n",
    "    assert len(ans) == len(pred)\n",
    "    return ans\n",
    "\n",
    "def splitSentence(st, pred):\n",
    "    temp_st = st\n",
    "    temp_pred = pred\n",
    "    temp_pred[0] = 2\n",
    "    buf = []\n",
    "    result = []\n",
    "    for (pre,char) in zip(temp_pred,temp_st):\n",
    "        if(pre == 0 or pre == 3):\n",
    "            result.append(buf)\n",
    "            buf = []\n",
    "        buf.append(char)\n",
    "    if(len(buf) > 0):\n",
    "        result.append(buf)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def sentencePrediction(dataset,prediction):\n",
    "    data_temp = dataset.copy()\n",
    "    assert len(data_temp) == len(prediction)\n",
    "    raw_sents = data_temp[\"raws\"]\n",
    "    assert len(raw_sents) == len(prediction)\n",
    "    ans = []\n",
    "    for (st,pred) in zip(raw_sents,prediction):\n",
    "        sptSt = splitSentence(st,pred)\n",
    "        ans.append(sptSt)\n",
    "    assert len(ans) == len(data_temp)\n",
    "    data_temp[\"tokenList\"]= ans\n",
    "    return data_temp\n",
    "    \n",
    "def convertToPredSts(dataset):\n",
    "    data_temp = dataset.copy()\n",
    "    tokenList = data_temp[\"tokenList\"]\n",
    "    ans = []\n",
    "    \n",
    "    for item in tokenList:\n",
    "        temp_st = list(map(lambda x : \"\".join(x), item))\n",
    "        temp = \"  \".join(temp_st)\n",
    "        temp = temp + \"  \"\n",
    "        ans.append(temp)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "result_pred = sentencePrediction(test_feature,preds)\n",
    "output_sts = convertToPredSts(result_pred)\n",
    "\n",
    "f = open(\"./ans.txt\", \"w\")\n",
    "f.write(\"\\n\".join(output_sts))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sts[452] == train_feature[\"input\"][452]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for con in range(len(output_sts)):\n",
    "    if not output_sts[con] == train_feature[\"input\"][con].replace(\"   \", \"  \"):\n",
    "        print()\n",
    "        print(train_feature[\"input\"][con])\n",
    "        \n",
    "        print(output_sts[con])\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13195449856465368577\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11182983223473196045\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5601390752\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10248998340463966654\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11306986678221424848\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
