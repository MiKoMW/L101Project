{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the google colab\n",
    "# !pip install xmnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "isColab = False\n",
    "googlePath = \"./drive/MyDrive/L101Project/\" if isColab else \"./\"\n",
    "cpuPools = 2 if isColab else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import Session\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = Session(config=config)\n",
    "# tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import xmnlp\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import collections\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkuTrain = googlePath + \"./corpus/cws/icwb2-data/training/pku_training.utf8\"\n",
    "pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "\n",
    "f = open(pkuTest, \"r\")\n",
    "temp = f.readlines()\n",
    "con = 0\n",
    "for item in temp:\n",
    "    con+=1\n",
    "    if(item == \"\\n\"):\n",
    "        print(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Corpus for Chinese Word Segamentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pkuTrain = googlePath + \"./corpus/cws/icwb2-data/training/pku_training.utf8\"\n",
    "pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "train = pd.read_table(pkuTrain,  encoding='utf8', header=None, names=['input'])  # don't drop the empty lines yet, they show up as NaN in the data frame\n",
    "# train.head(n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkuTest = googlePath  + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "test = pd.read_table(pkuTest,  encoding='utf8', header=None, names=['input']) \n",
    "test[\"raws\"] = test[\"input\"]\n",
    "# test.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19054\n"
     ]
    }
   ],
   "source": [
    "def prepross_data(train_Data):\n",
    "    inputs = train_Data[\"input\"][:]\n",
    "    print(len(inputs))\n",
    "    raws = []\n",
    "    tokenList = []\n",
    "    \n",
    "    for item in inputs:\n",
    "        item = item.replace(\"   \", \"  \")\n",
    "\n",
    "        raw = item.replace(\" \", \"\")\n",
    "#         print(item)\n",
    "#         print(raw)\n",
    "        raws.append(raw)\n",
    "        tokens = (item.split(\"  \")[:-1])\n",
    "        tokenList.append(tokens)\n",
    "#         print(tokens)\n",
    "    train_Data[\"raws\"] = raws\n",
    "    train_Data[\"tokenList\"] = tokenList\n",
    "    dictionary_train_word = list(set(reduce(operator.add,train_Data[\"tokenList\"])))\n",
    "    dictionary_train_char =  list(set(reduce(operator.add,train_Data[\"raws\"])))    \n",
    "    return train_Data, dictionary_train_char, dictionary_train_word\n",
    "train, train_dic_char, train_dic_word  = prepross_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lazy Load) Loading model...\n",
      "235\n"
     ]
    }
   ],
   "source": [
    "# Get redical dictionary\n",
    "\n",
    "# Get redical of a chinese character:\n",
    "def str_get_redical(st):\n",
    "    return xmnlp.radical(st)\n",
    "\n",
    "def get_redical_dic_from_char_dic(char_dic):\n",
    "    return list(set(str_get_redical(\"\".join(char_dic))))\n",
    "\n",
    "train_dic_redical = get_redical_dic_from_char_dic(train_dic_char)\n",
    "print(len(train_dic_redical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up subcharacter dictionary\n",
    "# Build up Chinese sub-character unites diactionary.\n",
    "chaiZi_Dic = {}\n",
    "with open(googlePath + \"corpus/chaizi/chaizi-jt.txt\", 'r') as f:\n",
    "    reader = csv.reader(f,delimiter='\\t')\n",
    "    for row in reader:\n",
    "        chaiZi_Dic[row[0]] = row[1]\n",
    "        \n",
    "def getSubChar(charCN):\n",
    "    if(charCN in chaiZi_Dic.keys()):\n",
    "        return chaiZi_Dic[charCN].split(\" \")\n",
    "#     elif charCN in train_dic_char:\n",
    "#         return [charCN]\n",
    "    else:\n",
    "        return [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1191\n"
     ]
    }
   ],
   "source": [
    "def str_get_subchar(st):\n",
    "#     for item in st:\n",
    "#         print(item)\n",
    "#         print(getSubChar(item))\n",
    "    return [getSubChar(item) for item in st]\n",
    "\n",
    "def get_subchar_dic_from_char_dic(char_dic):\n",
    "    return list(set(itertools.chain.from_iterable(str_get_subchar(\"\".join(char_dic)))))\n",
    "\n",
    "\n",
    "def get_top200_subchar_dic_from_char_dic(char_dic):\n",
    "    allsub = (list(itertools.chain.from_iterable(str_get_subchar(\"\".join(char_dic)))))\n",
    "    c = Counter(allsub)\n",
    "    ans = list(map(lambda x : x[0] , (c.most_common(200))))\n",
    "    return ans\n",
    "# train_dic_subchar = get_subchar_dic_from_char_dic(train_dic_char)\n",
    "train_dic_subchar_top200 = get_top200_subchar_dic_from_char_dic(train_dic_char)\n",
    "\n",
    "\n",
    "train_dic_subchar = get_subchar_dic_from_char_dic(train_dic_char)\n",
    "print(len(train_dic_subchar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the pre-processing\n",
    "def validate_data(data):\n",
    "    tkList = data[\"tokenList\"]\n",
    "    raws = data[\"raws\"]\n",
    "    for (tokens, raw) in zip(tkList,raws):\n",
    "#         print(tokens)\n",
    "        \n",
    "#         print(raw)\n",
    "        temp  = \"\".join(tokens)\n",
    "#         print(temp)\n",
    "        assert(temp == raw)\n",
    "        if not (temp == raw):\n",
    "            print(temp)\n",
    "            print(raw)\n",
    "            print(tokens)\n",
    "validate_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_dic_char))\n",
    "# print(len(set(train_dic_char)))\n",
    "# print(len(train_dic_word))\n",
    "# print(len(set(train_dic_word)))\n",
    "# with open(googlePath + \"./corpus/cws/icwb2-data/gold/pku_training_words.utf8\", 'r') as f:\n",
    "#     content = f.readlines()  \n",
    "#     print(len(content))\n",
    "\n",
    "# Make bag of subchar model.\n",
    "\n",
    "\n",
    "# \tletter = \n",
    "# \tletter[value] = 1\n",
    "\n",
    "\n",
    "\n",
    "# str_to_bagOfChar(\"迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）\")[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4698\n",
      "235\n",
      "1191\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "\n",
    "oov = len(train_dic_char)\n",
    "print(oov)\n",
    "\n",
    "oov_redical = len(train_dic_redical)\n",
    "print(oov_redical)\n",
    "\n",
    "oov_subchar = len(train_dic_subchar)\n",
    "print(oov_subchar)\n",
    "\n",
    "def token_index(tok):\n",
    "    ind = tok\n",
    "    if tok in train_dic_char:  # if token in vocabulary\n",
    "        ind = train_dic_char.index(tok)\n",
    "    else:  # else it's OOV\n",
    "        ind = oov\n",
    "    return ind\n",
    "\n",
    "def str_token_index(string):\n",
    "    return [token_index(x) for x in (string)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def redical_index(red):\n",
    "    ind = oov_redical\n",
    "    if red in train_dic_redical:  # if token in vocabulary\n",
    "        ind = train_dic_redical.index(red)\n",
    "    return ind\n",
    "\n",
    "def str_red_index(string):\n",
    "    return [redical_index(x) for x in str_get_redical(string)]\n",
    "\n",
    "\n",
    "def subchar_index(subchar):\n",
    "    ind = oov_subchar\n",
    "    if subchar in train_dic_subchar:\n",
    "        ind = train_dic_subchar.index(subchar)\n",
    "    return ind\n",
    "\n",
    "def str_subchar_index(string):\n",
    "    return [[subchar_index(temp) for temp in x] for x in str_get_subchar(string)]\n",
    "\n",
    "def char_to_bagOfChar(cnChar):\n",
    "    ans = [0 for _ in range(len(train_dic_subchar_top200)+1)]\n",
    "    for item in (getSubChar(cnChar)):\n",
    "        if item in train_dic_subchar_top200:\n",
    "            idx = train_dic_subchar_top200.index(item)\n",
    "        else:\n",
    "            idx = len(train_dic_subchar_top200)\n",
    "        ans[idx] += 1\n",
    "    return (ans)\n",
    "\n",
    "def str_to_bagOfChar(cnStr):\n",
    "    return [char_to_bagOfChar(cnChar) for cnChar in cnStr]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Get Begin Middle End Single sequence\n",
    "# B 0 M 1 E 2 S 3\n",
    "\n",
    "def str_bmes_idx(tokenList):\n",
    "    answer = []\n",
    "    for item in tokenList:\n",
    "        if len(item) == 0:\n",
    "            raise NameErro(\"Zero Length Word\")\n",
    "        if len(item) == 1:\n",
    "            answer.append(3)\n",
    "        else:\n",
    "            answer.append(0)\n",
    "            for item in range(len(item) - 2):\n",
    "                answer.append(1)\n",
    "            answer.append(2)\n",
    "    return answer\n",
    "\n",
    "def extract_features(data_set, isTest=False):\n",
    "    data_temp = data_set.copy()\n",
    "\n",
    "    # Idx for chars\n",
    "    with Pool(8) as p:\n",
    "        tokinds = p.map(str_token_index,data_temp['raws'])\n",
    "#     tokinds = [list(map(token_index, u)) for u in data_temp['raws']]\n",
    "\n",
    "    with Pool(8) as p:\n",
    "        redinds = p.map(str_red_index,data_temp['raws'])\n",
    "        \n",
    "    with Pool(8) as p:\n",
    "        subcharidx = p.map(str_subchar_index,data_temp['raws'])\n",
    "        \n",
    "#     subcharidx_bag = list(m0ap(str_to_bagOfChar,data_temp['raws']))\n",
    "        \n",
    "\n",
    "    data_temp[\"tokenIdx\"] = tokinds\n",
    "    data_temp[\"redIdx\"] = redinds\n",
    "    data_temp[\"subcharIdx\"] = subcharidx\n",
    "#     data_temp[\"subcharIdxBag\"] = subcharidx_bag\n",
    "    \n",
    "    # BIO\n",
    "    if(not isTest):\n",
    "        data_temp[\"bmes\"] = [str_bmes_idx(u) for u in data_temp['tokenList']]\n",
    "        assert (list(map(len,data_temp[\"bmes\"])) == list(map(len,data_temp[\"tokenIdx\"])))\n",
    "    \n",
    "    return data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 s, sys: 304 ms, total: 2.82 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MultiThreading\n",
    "train_feature = extract_features(train)\n",
    "# train_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenList</th>\n",
       "      <th>tokenIdx</th>\n",
       "      <th>redIdx</th>\n",
       "      <th>subcharIdx</th>\n",
       "      <th>bmes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...</td>\n",
       "      <td>迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）</td>\n",
       "      <td>[迈向, 充满, 希望, 的, 新, 世纪, ——, 一九九八年, 新年, 讲话, （, 附...</td>\n",
       "      <td>[2820, 2896, 2538, 1719, 4222, 983, 3642, 3029...</td>\n",
       "      <td>[168, 64, 34, 26, 192, 228, 73, 36, 6, 28, 195...</td>\n",
       "      <td>[[608, 393], [154, 22, 1001], [1188, 157], [83...</td>\n",
       "      <td>[0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 2, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...   \n",
       "\n",
       "                           raws  \\\n",
       "0  迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）   \n",
       "\n",
       "                                           tokenList  \\\n",
       "0  [迈向, 充满, 希望, 的, 新, 世纪, ——, 一九九八年, 新年, 讲话, （, 附...   \n",
       "\n",
       "                                            tokenIdx  \\\n",
       "0  [2820, 2896, 2538, 1719, 4222, 983, 3642, 3029...   \n",
       "\n",
       "                                              redIdx  \\\n",
       "0  [168, 64, 34, 26, 192, 228, 73, 36, 6, 28, 195...   \n",
       "\n",
       "                                          subcharIdx  \\\n",
       "0  [[608, 393], [154, 22, 1001], [1188, 157], [83...   \n",
       "\n",
       "                                                bmes  \n",
       "0  [0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 2, 0, 1, 1, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_sequence(data_with_features):\n",
    "#     assert (np.max(list(map(len, data_with_features[\"tokenIdx\"])))) == (np.max(list(map(len, data_with_features[\"bmes\"]))))\n",
    "    return (np.max(list(map(len, data_with_features[\"tokenIdx\"]))))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 153 ms, sys: 220 ms, total: 373 ms\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = extract_features(test, isTest=True)\n",
    "test_feature = temp\n",
    "# test_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenIdx</th>\n",
       "      <th>redIdx</th>\n",
       "      <th>subcharIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n",
       "      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n",
       "      <td>[592, 2632, 3405, 2988, 4161, 1081, 3642, 3029...</td>\n",
       "      <td>[125, 64, 33, 168, 196, 214, 73, 36, 6, 28, 19...</td>\n",
       "      <td>[[771, 164], [22, 923, 1001], [636, 282], [608...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>（二○○○年十二月三十一日）（附图片1张）</td>\n",
       "      <td>（二○○○年十二月三十一日）（附图片1张）</td>\n",
       "      <td>[4390, 2666, 735, 735, 735, 1740, 1585, 2666, ...</td>\n",
       "      <td>[195, 24, 195, 195, 195, 146, 81, 24, 228, 6, ...</td>\n",
       "      <td>[[262], [923, 923], [262], [262], [262], [537,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女士们，先生们，同志们，朋友们：</td>\n",
       "      <td>女士们，先生们，同志们，朋友们：</td>\n",
       "      <td>[1438, 1228, 1236, 2207, 1969, 1454, 1236, 220...</td>\n",
       "      <td>[214, 42, 92, 195, 34, 229, 92, 195, 64, 153, ...</td>\n",
       "      <td>[[561, 923, 537], [440, 923], [306, 142], [262...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...</td>\n",
       "      <td>2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...</td>\n",
       "      <td>[4191, 546, 546, 2763, 1740, 3029, 1740, 2546,...</td>\n",
       "      <td>[195, 195, 195, 195, 146, 36, 146, 84, 42, 225...</td>\n",
       "      <td>[[262], [262], [262], [262], [537, 255, 154], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...</td>\n",
       "      <td>在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...</td>\n",
       "      <td>[2203, 3380, 167, 3536, 2848, 3787, 2527, 3642...</td>\n",
       "      <td>[1, 168, 162, 26, 32, 231, 153, 73, 111, 33, 1...</td>\n",
       "      <td>[[923, 306, 913], [608, 325], [306, 813], [830...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                              共同创造美好的新世纪——二○○一年新年贺词   \n",
       "1                              （二○○○年十二月三十一日）（附图片1张）   \n",
       "2                                   女士们，先生们，同志们，朋友们：   \n",
       "3  2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...   \n",
       "4  在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...   \n",
       "\n",
       "                                                raws  \\\n",
       "0                              共同创造美好的新世纪——二○○一年新年贺词   \n",
       "1                              （二○○○年十二月三十一日）（附图片1张）   \n",
       "2                                   女士们，先生们，同志们，朋友们：   \n",
       "3  2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...   \n",
       "4  在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...   \n",
       "\n",
       "                                            tokenIdx  \\\n",
       "0  [592, 2632, 3405, 2988, 4161, 1081, 3642, 3029...   \n",
       "1  [4390, 2666, 735, 735, 735, 1740, 1585, 2666, ...   \n",
       "2  [1438, 1228, 1236, 2207, 1969, 1454, 1236, 220...   \n",
       "3  [4191, 546, 546, 2763, 1740, 3029, 1740, 2546,...   \n",
       "4  [2203, 3380, 167, 3536, 2848, 3787, 2527, 3642...   \n",
       "\n",
       "                                              redIdx  \\\n",
       "0  [125, 64, 33, 168, 196, 214, 73, 36, 6, 28, 19...   \n",
       "1  [195, 24, 195, 195, 195, 146, 81, 24, 228, 6, ...   \n",
       "2  [214, 42, 92, 195, 34, 229, 92, 195, 64, 153, ...   \n",
       "3  [195, 195, 195, 195, 146, 36, 146, 84, 42, 225...   \n",
       "4  [1, 168, 162, 26, 32, 231, 153, 73, 111, 33, 1...   \n",
       "\n",
       "                                          subcharIdx  \n",
       "0  [[771, 164], [22, 923, 1001], [636, 282], [608...  \n",
       "1  [[262], [923, 923], [262], [262], [262], [537,...  \n",
       "2  [[561, 923, 537], [440, 923], [306, 142], [262...  \n",
       "3  [[262], [262], [262], [262], [537, 255, 154], ...  \n",
       "4  [[923, 306, 913], [608, 325], [306, 813], [830...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n",
      "626\n",
      "1019\n"
     ]
    }
   ],
   "source": [
    "train_longest = find_longest_sequence(train_feature)\n",
    "print(train_longest)\n",
    "test_longest = find_longest_sequence(test_feature)\n",
    "print(test_longest)\n",
    "seq_longest = np.max([train_longest,test_longest])\n",
    "print(seq_longest)\n",
    "\n",
    "subchar_seq_length = np.max(list(map(lambda x : len(getSubChar(x)),train_dic_char)))\n",
    "subchar_seq_length = 3\n",
    "subsubchar_padtok = oov_subchar + 1\n",
    "seq_length = seq_longest\n",
    "\n",
    "def padd_char(seq):\n",
    "    temp_char_seqs_padded = []\n",
    "    for item in seq[\"subcharIdx\"]:\n",
    "        temp_pad = pad_sequences(item, maxlen=subchar_seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=subsubchar_padtok)\n",
    "  \n",
    "        a = temp_pad\n",
    "        b = [[subsubchar_padtok for i in range(subchar_seq_length)] for _ in range(0, seq_length - len(temp_pad))]\n",
    "        if len(b) == 0:\n",
    "            c = a\n",
    "        else:\n",
    "            c = np.concatenate((a, b))\n",
    "        \n",
    "    # print(len(c))\n",
    "        temp_char_seqs_padded.append(c)\n",
    "  # print(len(temp_char_seqs_padded))\n",
    "    return temp_char_seqs_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subchar_seqs_padded = padd_char(train_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padding token index is 4699\n",
      "Example of padded token sequence:\n",
      "[3548  592 3548 ... 4699 4699 4699]\n"
     ]
    }
   ],
   "source": [
    "seq_length = seq_longest\n",
    "\n",
    "# a new dummy token index, one more than OOV\n",
    "padtok = oov+1\n",
    "red_padtok = oov_redical + 1\n",
    "print('The padding token index is %i' % padtok)\n",
    "\n",
    "# use pad_sequences, padding or truncating at the end of the sequence (default is 'pre')\n",
    "train_seqs_padded = pad_sequences(train_feature['tokenIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
    "print('Example of padded token sequence:')\n",
    "print(train_seqs_padded[1])\n",
    "\n",
    "\n",
    "train_red_padded = pad_sequences(train_feature['redIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=red_padtok)\n",
    "\n",
    "\n",
    "train_subchar_seqs_padded = padd_char(train_feature)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare Test set.\n",
    "test_seqs_padded = pad_sequences(test_feature['tokenIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
    "\n",
    "test_rad_padded = pad_sequences(test_feature['redIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=red_padtok)\n",
    "\n",
    "test_subchar_seqs_padded = padd_char(test_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lists of named entity labels, padded with a null label (=3)\n",
    "padlab = 4\n",
    "train_labs_padded = pad_sequences(train_feature['bmes'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padlab)\n",
    "\n",
    "# convert those labels to one-hot encoding\n",
    "n_labs = 5\n",
    "train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]\n",
    "\n",
    "# # follow the print outputs below to see how the labels are transformed\n",
    "# print('Length of input sequence: %i' % len(train_labs_padded[1]))\n",
    "# print('Length of label sequence: %i' % len(train_labs_onehot[1]))\n",
    "# print(train_labs_padded[1][:11])\n",
    "# print(train_labs_onehot[1][:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "**Defining a neural network**\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input3 (InputLayer)        [(None, 1019, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tok_input1 (InputLayer)         [(None, 1019)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "red_input2 (InputLayer)         [(None, 1019)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 1019, 3, 128) 152704      char_input3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1019, 128)    601600      tok_input1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1019, 128)    30336       red_input2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 1019, 100)    71600       time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1019, 356)    0           embedding_9[0][0]                \n",
      "                                                                 embedding_10[0][0]               \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1019, 100)    162800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1019, 100)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 1019, 5)      505         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,019,545\n",
      "Trainable params: 1,019,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load Keras and TensorFlow\n",
    "\n",
    "\n",
    "\n",
    "# our final vocab size is the padding token + 1 (OR length of vocab + OOV + PAD)\n",
    "vocab_size = padtok+1\n",
    "red_size = red_padtok+1\n",
    "subchar_size = subsubchar_padtok + 1\n",
    "\n",
    "print(vocab_size==len(train_dic_char)+2)\n",
    "embed_size = 128 # y an embedding size of 128 (could tune this)\n",
    "\n",
    "# list of metrics to use: true & false positives, negatives, accuracy, precision, recall, area under the curve\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "# our model has the option for an label prediction bias, it's sequential, starts with an embedding layer, then bi-LSTM,\n",
    "# a dropout layer follows for regularisation, and a dense final layer with softmax activation to output class probabilities\n",
    "# we compile with the Adam optimizer at a low learning rate, use categorical cross-entropy as our loss function\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:        \n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    tok_input1 = keras.layers.Input(shape=(seq_length,), dtype='int32', name='tok_input1')\n",
    "    red_input2 = keras.layers.Input(shape=(seq_length,), dtype='int32', name='red_input2')\n",
    "    subchar_input3 = keras.layers.Input(shape=(seq_length,subchar_seq_length), dtype='int32', name='char_input3')\n",
    "\n",
    "    emb_char = keras.layers.TimeDistributed(keras.layers.Embedding(output_dim=embed_size, input_dim=subchar_size, input_length=3,  mask_zero=True, trainable=True))(subchar_input3)\n",
    "#     emb_char = keras.layers.TimeDistributed(keras.layers.Embedding(output_dim=embed_size, input_dim=subchar_size, input_length=3,  mask_zero=True, trainable=True))(subchar_input3)\n",
    "    \n",
    "    char_enc = keras.layers.TimeDistributed(keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=False, dropout=0.2,recurrent_dropout=0.2)))(emb_char)\n",
    "        \n",
    "\n",
    "    x1 = keras.layers.Embedding(output_dim=embed_size, input_dim=vocab_size,  input_length=seq_length,  mask_zero=True, trainable=True)(tok_input1)\n",
    "    x2 = keras.layers.Embedding(output_dim=embed_size, input_dim=red_size,  input_length=seq_length, mask_zero=True, trainable=True)(red_input2)\n",
    "    x_cancat = keras.layers.concatenate([x1, x2, char_enc])\n",
    "#     x_cancat = (x1)\n",
    "    x_lstm = keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2))(x_cancat)\n",
    "    x_drop = keras.layers.Dropout(0.5)(x_lstm)\n",
    "    main_output = keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias))(x_drop)\n",
    "    model = keras.models.Model(inputs=[tok_input1,red_input2, subchar_input3], outputs= main_output)\n",
    "#     model = keras.models.Model(inputs=[tok_input1, pos_input2, char_input3], outputs= main_output)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Old lstm model\n",
    "#     model = keras.Sequential([\n",
    "#         keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True),\n",
    "#         keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2)),  # 2 directions, 50 units each, concatenated (can change this)\n",
    "#         keras.layers.Dropout(0.5),\n",
    "#         keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias)),\n",
    "#     ])\n",
    "\n",
    "\n",
    "# early stopping criteria based on area under the curve: will stop if no improvement after 10 epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n",
    "\n",
    "# the number of training epochs we'll use, and the batch size (how many texts are input at once)\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "print('**Defining a neural network**')\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # evaluate our initial model\n",
    "# results = model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=0)\n",
    "# print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 17589578, 0: 585226, 2: 585226, 3: 524721, 1: 131275})\n",
      "19416026\n",
      "Initial bias:\n",
      "[0.03014138938627297, 0.006761167295511451, 0.03014138938627297, 0.027025149224666263, 0.9059309047072763]\n"
     ]
    }
   ],
   "source": [
    "# figure out the label distribution in our fixed-length texts\n",
    "from collections import Counter\n",
    "all_labs = [l for lab in train_labs_padded for l in lab]\n",
    "label_count = Counter(all_labs)\n",
    "total_labs = len(all_labs)\n",
    "print(label_count)\n",
    "print(total_labs)\n",
    "\n",
    "# use this to define an initial model bias\n",
    "initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n",
    "              (label_count[2]/total_labs), (label_count[3]/total_labs), (label_count[4]/total_labs)]\n",
    "print('Initial bias:')\n",
    "print(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence dimensions (n.docs, seq.length):\n",
      "(19054, 1019)\n",
      "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
      "(19054, 1019, 5)\n"
     ]
    }
   ],
   "source": [
    "# prepare sequences and labels as numpy arrays, check dimensions\n",
    "X = np.array(train_seqs_padded)\n",
    "X_red = np.array(train_red_padded)\n",
    "X_sub = np.array(train_subchar_seqs_padded)\n",
    "y = np.array(train_labs_onehot)\n",
    "print('Input sequence dimensions (n.docs, seq.length):')\n",
    "print(X.shape)\n",
    "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "13337/13337 [==============================] - 994s 75ms/step - loss: 0.0376 - tp: 13368166.0000 - fp: 134333.0000 - tn: 54227164.0000 - fn: 222237.0000 - accuracy: 0.9948 - precision: 0.9901 - recall: 0.9836 - auc: 0.9997 - val_loss: 0.0258 - val_tp: 5766751.0000 - val_fp: 44656.0000 - val_tn: 23257868.0000 - val_fn: 58872.0000 - val_accuracy: 0.9964 - val_precision: 0.9923 - val_recall: 0.9899 - val_auc: 0.9998\n",
      "Epoch 2/10\n",
      "13337/13337 [==============================] - 1009s 76ms/step - loss: 0.0246 - tp: 13460860.0000 - fp: 97751.0000 - tn: 54263776.0000 - fn: 129543.0000 - accuracy: 0.9967 - precision: 0.9928 - recall: 0.9905 - auc: 0.9998 - val_loss: 0.0221 - val_tp: 5775202.0000 - val_fp: 38601.0000 - val_tn: 23263868.0000 - val_fn: 50421.0000 - val_accuracy: 0.9969 - val_precision: 0.9934 - val_recall: 0.9913 - val_auc: 0.9998\n",
      "Epoch 3/10\n",
      "13337/13337 [==============================] - 1080s 81ms/step - loss: 0.0215 - tp: 13478432.0000 - fp: 86595.0000 - tn: 54275208.0000 - fn: 111971.0000 - accuracy: 0.9971 - precision: 0.9936 - recall: 0.9918 - auc: 0.9998 - val_loss: 0.0207 - val_tp: 5779908.0000 - val_fp: 36163.0000 - val_tn: 23266344.0000 - val_fn: 45715.0000 - val_accuracy: 0.9972 - val_precision: 0.9938 - val_recall: 0.9922 - val_auc: 0.9998\n",
      "Epoch 4/10\n",
      "13337/13337 [==============================] - 1009s 76ms/step - loss: 0.0198 - tp: 13487795.0000 - fp: 80330.0000 - tn: 54281544.0000 - fn: 102608.0000 - accuracy: 0.9973 - precision: 0.9941 - recall: 0.9924 - auc: 0.9998 - val_loss: 0.0200 - val_tp: 5782326.0000 - val_fp: 35732.0000 - val_tn: 23266804.0000 - val_fn: 43297.0000 - val_accuracy: 0.9973 - val_precision: 0.9939 - val_recall: 0.9926 - val_auc: 0.9998\n",
      "Epoch 5/10\n",
      "13337/13337 [==============================] - 1009s 76ms/step - loss: 0.0186 - tp: 13494582.0000 - fp: 75992.0000 - tn: 54285872.0000 - fn: 95821.0000 - accuracy: 0.9975 - precision: 0.9944 - recall: 0.9929 - auc: 0.9998 - val_loss: 0.0194 - val_tp: 5784415.0000 - val_fp: 34154.0000 - val_tn: 23268356.0000 - val_fn: 41208.0000 - val_accuracy: 0.9974 - val_precision: 0.9941 - val_recall: 0.9929 - val_auc: 0.9997\n",
      "Epoch 6/10\n",
      "13337/13337 [==============================] - 1017s 76ms/step - loss: 0.0177 - tp: 13499400.0000 - fp: 72527.0000 - tn: 54289432.0000 - fn: 91003.0000 - accuracy: 0.9976 - precision: 0.9947 - recall: 0.9933 - auc: 0.9998 - val_loss: 0.0192 - val_tp: 5785600.0000 - val_fp: 33840.0000 - val_tn: 23268700.0000 - val_fn: 40023.0000 - val_accuracy: 0.9975 - val_precision: 0.9942 - val_recall: 0.9931 - val_auc: 0.9997\n",
      "Epoch 7/10\n",
      "13337/13337 [==============================] - 1017s 76ms/step - loss: 0.0170 - tp: 13502869.0000 - fp: 70393.0000 - tn: 54291620.0000 - fn: 87534.0000 - accuracy: 0.9977 - precision: 0.9948 - recall: 0.9936 - auc: 0.9998 - val_loss: 0.0192 - val_tp: 5786020.0000 - val_fp: 33663.0000 - val_tn: 23268896.0000 - val_fn: 39603.0000 - val_accuracy: 0.9975 - val_precision: 0.9942 - val_recall: 0.9932 - val_auc: 0.9997\n",
      "Epoch 8/10\n",
      "13337/13337 [==============================] - 1014s 76ms/step - loss: 0.0166 - tp: 13505561.0000 - fp: 68498.0000 - tn: 54293540.0000 - fn: 84842.0000 - accuracy: 0.9977 - precision: 0.9950 - recall: 0.9938 - auc: 0.9998 - val_loss: 0.0186 - val_tp: 5787048.0000 - val_fp: 32394.0000 - val_tn: 23270068.0000 - val_fn: 38575.0000 - val_accuracy: 0.9976 - val_precision: 0.9944 - val_recall: 0.9934 - val_auc: 0.9997\n",
      "Epoch 9/10\n",
      "13337/13337 [==============================] - 1018s 76ms/step - loss: 0.0161 - tp: 13507787.0000 - fp: 66839.0000 - tn: 54295156.0000 - fn: 82616.0000 - accuracy: 0.9978 - precision: 0.9951 - recall: 0.9939 - auc: 0.9999 - val_loss: 0.0187 - val_tp: 5787279.0000 - val_fp: 32776.0000 - val_tn: 23269756.0000 - val_fn: 38344.0000 - val_accuracy: 0.9976 - val_precision: 0.9944 - val_recall: 0.9934 - val_auc: 0.9997\n",
      "Epoch 10/10\n",
      "13337/13337 [==============================] - 1009s 76ms/step - loss: 0.0158 - tp: 13509213.0000 - fp: 65814.0000 - tn: 54296192.0000 - fn: 81190.0000 - accuracy: 0.9978 - precision: 0.9952 - recall: 0.9940 - auc: 0.9999 - val_loss: 0.0184 - val_tp: 5787936.0000 - val_fp: 31945.0000 - val_tn: 23270572.0000 - val_fn: 37687.0000 - val_accuracy: 0.9976 - val_precision: 0.9945 - val_recall: 0.9935 - val_auc: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faa63b03e10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-initiate model with bias\n",
    "model = make_model(output_bias=initial_bias)\n",
    "# and fit...\n",
    "model.fit([X,X_red,X_sub], y, batch_size=128, epochs=10, callbacks = [early_stopping],  validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 1808203, 2: 57461, 0: 57383, 3: 45354, 1: 12535})\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(test_seqs_padded)\n",
    "X_test_rad = np.array(test_rad_padded)\n",
    "X_test_subchar = np.array(test_subchar_seqs_padded)\n",
    "\n",
    "# preds = np.argmax(model.predict(X_test), axis=-1)\n",
    "preds = np.argmax(model.predict([X_test,X_test_rad,X_test_subchar]), axis=-1)\n",
    "flat_preds = [p for pred in preds for p in pred]\n",
    "print(Counter(flat_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1944"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def postEditPred(pred):\n",
    "    print(pred)\n",
    "    ans = []\n",
    "    for item in pred:\n",
    "        if item == 0 or item == 3:\n",
    "            ans.append(1)\n",
    "        else:\n",
    "            ans.append(0)\n",
    "            \n",
    "    assert len(ans) == len(pred)\n",
    "    return ans\n",
    "\n",
    "def splitSentence(st, pred):\n",
    "    temp_st = st\n",
    "    temp_pred = pred\n",
    "    temp_pred[0] = 2\n",
    "    buf = []\n",
    "    result = []\n",
    "    for (pre,char) in zip(temp_pred,temp_st):\n",
    "        if(pre == 0 or pre == 3):\n",
    "            result.append(buf)\n",
    "            buf = []\n",
    "        buf.append(char)\n",
    "    if(len(buf) > 0):\n",
    "        result.append(buf)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def sentencePrediction(dataset,prediction):\n",
    "    data_temp = dataset.copy()\n",
    "    assert len(data_temp) == len(prediction)\n",
    "    raw_sents = data_temp[\"raws\"]\n",
    "    assert len(raw_sents) == len(prediction)\n",
    "    ans = []\n",
    "    for (st,pred) in zip(raw_sents,prediction):\n",
    "        sptSt = splitSentence(st,pred)\n",
    "        ans.append(sptSt)\n",
    "    assert len(ans) == len(data_temp)\n",
    "    data_temp[\"tokenList\"]= ans\n",
    "    return data_temp\n",
    "    \n",
    "def convertToPredSts(dataset):\n",
    "    data_temp = dataset.copy()\n",
    "    tokenList = data_temp[\"tokenList\"]\n",
    "    ans = []\n",
    "    \n",
    "    for item in tokenList:\n",
    "        temp_st = list(map(lambda x : \"\".join(x), item))\n",
    "        temp = \"  \".join(temp_st)\n",
    "        temp = temp + \"  \"\n",
    "        ans.append(temp)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pred = sentencePrediction(test_feature,preds)\n",
    "output_sts = convertToPredSts(result_pred)\n",
    "\n",
    "f = open(\"./ans.txt\", \"w\")\n",
    "f.write(\"\\n\".join(output_sts))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(googlePath + \"./this.txt\", \"w\")\n",
    "f.write(\"\\n\".join(output_sts))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
