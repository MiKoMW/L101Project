{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the google colab\n",
    "# !pip install xmnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "isColab = False\n",
    "\n",
    "googlePath = \"./drive/MyDrive/L101Project/\" if isColab else \"./\"\n",
    "cpuPools = 2 if isColab else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import Session\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = Session(config=config)\n",
    "# tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import xmnlp\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import collections\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkuTrain = googlePath + \"./corpus/cws/icwb2-data/training/pku_training.utf8\"\n",
    "pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "\n",
    "f = open(pkuTest, \"r\")\n",
    "temp = f.readlines()\n",
    "con = 0\n",
    "for item in temp:\n",
    "    con+=1\n",
    "    if(item == \"\\n\"):\n",
    "        print(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Corpus for Chinese Word Segamentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pkuTrain = googlePath + \"./corpus/cws/icwb2-data/training/pku_training.utf8\"\n",
    "pkuTest = googlePath + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "train = pd.read_table(pkuTrain,  encoding='utf8', header=None, names=['input'])  # don't drop the empty lines yet, they show up as NaN in the data frame\n",
    "# train.head(n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkuTest = googlePath  + \"./corpus/cws/icwb2-data/testing/pku_test.utf8\"\n",
    "test = pd.read_table(pkuTest,  encoding='utf8', header=None, names=['input']) \n",
    "test[\"raws\"] = test[\"input\"]\n",
    "# test.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19054\n"
     ]
    }
   ],
   "source": [
    "def prepross_data(train_Data):\n",
    "    inputs = train_Data[\"input\"][:]\n",
    "    print(len(inputs))\n",
    "    raws = []\n",
    "    tokenList = []\n",
    "    \n",
    "    for item in inputs:\n",
    "        item = item.replace(\"   \", \"  \")\n",
    "\n",
    "        raw = item.replace(\" \", \"\")\n",
    "#         print(item)\n",
    "#         print(raw)\n",
    "        raws.append(raw)\n",
    "        tokens = (item.split(\"  \")[:-1])\n",
    "        tokenList.append(tokens)\n",
    "#         print(tokens)\n",
    "    train_Data[\"raws\"] = raws\n",
    "    train_Data[\"tokenList\"] = tokenList\n",
    "    dictionary_train_word = list(set(reduce(operator.add,train_Data[\"tokenList\"])))\n",
    "    dictionary_train_char =  list(set(reduce(operator.add,train_Data[\"raws\"])))    \n",
    "    return train_Data, dictionary_train_char, dictionary_train_word\n",
    "train, train_dic_char, train_dic_word  = prepross_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lazy Load) Loading model...\n",
      "235\n"
     ]
    }
   ],
   "source": [
    "# Get redical dictionary\n",
    "\n",
    "# Get redical of a chinese character:\n",
    "def str_get_redical(st):\n",
    "    return xmnlp.radical(st)\n",
    "\n",
    "def get_redical_dic_from_char_dic(char_dic):\n",
    "    return list(set(str_get_redical(\"\".join(char_dic))))\n",
    "\n",
    "train_dic_redical = get_redical_dic_from_char_dic(train_dic_char)\n",
    "print(len(train_dic_redical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up subcharacter dictionary\n",
    "# Build up Chinese sub-character unites diactionary.\n",
    "chaiZi_Dic = {}\n",
    "with open(googlePath + \"corpus/chaizi/chaizi-jt.txt\", 'r') as f:\n",
    "    reader = csv.reader(f,delimiter='\\t')\n",
    "    for row in reader:\n",
    "        chaiZi_Dic[row[0]] = row[1]\n",
    "        \n",
    "def getSubChar(charCN):\n",
    "    if(charCN in chaiZi_Dic.keys()):\n",
    "        return chaiZi_Dic[charCN].split(\" \")\n",
    "#     elif charCN in train_dic_char:\n",
    "#         return [charCN]\n",
    "    else:\n",
    "        return [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "def str_get_subchar(st):\n",
    "#     for item in st:\n",
    "#         print(item)\n",
    "#         print(getSubChar(item))\n",
    "    return [getSubChar(item) for item in st]\n",
    "\n",
    "def get_subchar_dic_from_char_dic(char_dic):\n",
    "    return list(set(itertools.chain.from_iterable(str_get_subchar(\"\".join(char_dic)))))\n",
    "\n",
    "def get_top200_subchar_dic_from_char_dic(char_dic):\n",
    "    allsub = (list(itertools.chain.from_iterable(str_get_subchar(\"\".join(char_dic)))))\n",
    "    c = Counter(allsub)\n",
    "    ans = list(map(lambda x : x[0] , (c.most_common(200))))\n",
    "    return ans\n",
    "# train_dic_subchar = get_subchar_dic_from_char_dic(train_dic_char)\n",
    "train_dic_subchar = get_top200_subchar_dic_from_char_dic(train_dic_char)\n",
    "\n",
    "\n",
    "print(len(train_dic_subchar))\n",
    "# print((train_dic_subchar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the pre-processing\n",
    "def validate_data(data):\n",
    "    tkList = data[\"tokenList\"]\n",
    "    raws = data[\"raws\"]\n",
    "    for (tokens, raw) in zip(tkList,raws):\n",
    "#         print(tokens)\n",
    "        \n",
    "#         print(raw)\n",
    "        temp  = \"\".join(tokens)\n",
    "#         print(temp)\n",
    "        assert(temp == raw)\n",
    "        if not (temp == raw):\n",
    "            print(temp)\n",
    "            print(raw)\n",
    "            print(tokens)\n",
    "validate_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_dic_char))\n",
    "# print(len(set(train_dic_char)))\n",
    "# print(len(train_dic_word))\n",
    "# print(len(set(train_dic_word)))\n",
    "# with open(googlePath + \"./corpus/cws/icwb2-data/gold/pku_training_words.utf8\", 'r') as f:\n",
    "#     content = f.readlines()  \n",
    "#     print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4698\n",
      "235\n",
      "1191\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "\n",
    "oov = len(train_dic_char)\n",
    "print(oov)\n",
    "\n",
    "oov_redical = len(train_dic_redical)\n",
    "print(oov_redical)\n",
    "\n",
    "oov_subchar = len(train_dic_subchar)\n",
    "print(oov_subchar)\n",
    "\n",
    "def token_index(tok):\n",
    "    ind = tok\n",
    "    if tok in train_dic_char:  # if token in vocabulary\n",
    "        ind = train_dic_char.index(tok)\n",
    "    else:  # else it's OOV\n",
    "        ind = oov\n",
    "    return ind\n",
    "\n",
    "def str_token_index(string):\n",
    "    return [token_index(x) for x in (string)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def redical_index(red):\n",
    "    ind = oov_redical\n",
    "    if red in train_dic_redical:  # if token in vocabulary\n",
    "        ind = train_dic_redical.index(red)\n",
    "    return ind\n",
    "\n",
    "def str_red_index(string):\n",
    "    return [redical_index(x) for x in str_get_redical(string)]\n",
    "\n",
    "\n",
    "def subchar_index(subchar):\n",
    "    ind = oov_subchar\n",
    "    if subchar in train_dic_subchar:\n",
    "        ind = train_dic_subchar.index(subchar)\n",
    "    return ind\n",
    "\n",
    "def str_subchar_index(string):\n",
    "    return [[subchar_index(temp) for temp in x] for x in str_get_subchar(string)]\n",
    "\n",
    "\n",
    "# Get Begin Middle End Single sequence\n",
    "# B 0 M 1 E 2 S 3\n",
    "\n",
    "def str_bmes_idx(tokenList):\n",
    "    answer = []\n",
    "    for item in tokenList:\n",
    "        if len(item) == 0:\n",
    "            raise NameErro(\"Zero Length Word\")\n",
    "        if len(item) == 1:\n",
    "            answer.append(3)\n",
    "        else:\n",
    "            answer.append(0)\n",
    "            for item in range(len(item) - 2):\n",
    "                answer.append(1)\n",
    "            answer.append(2)\n",
    "    return answer\n",
    "\n",
    "def extract_features(data_set, isTest=False):\n",
    "    data_temp = data_set.copy()\n",
    "\n",
    "    # Idx for chars\n",
    "    with Pool(8) as p:\n",
    "        tokinds = p.map(str_token_index,data_temp['raws'])\n",
    "#     tokinds = [list(map(token_index, u)) for u in data_temp['raws']]\n",
    "\n",
    "    with Pool(8) as p:\n",
    "        redinds = p.map(str_red_index,data_temp['raws'])\n",
    "        \n",
    "    with Pool(8) as p:\n",
    "        subcharidx = p.map(str_subchar_index,data_temp['raws'])\n",
    "        \n",
    "\n",
    "    data_temp[\"tokenIdx\"] = tokinds\n",
    "    data_temp[\"redIdx\"] = redinds\n",
    "    data_temp[\"subcharIdx\"] = subcharidx\n",
    "    \n",
    "    # BIO\n",
    "    if(not isTest):\n",
    "        data_temp[\"bmes\"] = [str_bmes_idx(u) for u in data_temp['tokenList']]\n",
    "        assert (list(map(len,data_temp[\"bmes\"])) == list(map(len,data_temp[\"tokenIdx\"])))\n",
    "\n",
    "        \n",
    "#     print(data_temp[\"bmes\"])\n",
    "#    assert reduce(operator.and_, list(map(len,data_temp[\"bmes\"])) == list(map(len,data_temp[\"tokenIdx\"])))\n",
    "    \n",
    "    # Get redical\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return data_temp\n",
    "#     txt['token_indices'] = tokinds\n",
    "#     if not istest:  # can't do this with the test set\n",
    "#         bioints = [bio_index(b) for b in txt['bio_only']]\n",
    "#         txt['bio_only'] = bioints\n",
    "#     return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.43 s, sys: 396 ms, total: 3.82 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MultiThreading\n",
    "train_feature = extract_features(train)\n",
    "# train_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenList</th>\n",
       "      <th>tokenIdx</th>\n",
       "      <th>redIdx</th>\n",
       "      <th>subcharIdx</th>\n",
       "      <th>bmes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...</td>\n",
       "      <td>迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）</td>\n",
       "      <td>[迈向, 充满, 希望, 的, 新, 世纪, ——, 一九九八年, 新年, 讲话, （, 附...</td>\n",
       "      <td>[2189, 2033, 1116, 1566, 88, 379, 1461, 989, 2...</td>\n",
       "      <td>[171, 90, 88, 58, 80, 94, 133, 165, 124, 107, ...</td>\n",
       "      <td>[[521, 1143], [1160, 254, 698], [1007, 986], [...</td>\n",
       "      <td>[0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 2, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  迈向  充满  希望  的  新  世纪  ——  一九九八年  新年  讲话  （  附 ...   \n",
       "\n",
       "                           raws  \\\n",
       "0  迈向充满希望的新世纪——一九九八年新年讲话（附图片１张）   \n",
       "\n",
       "                                           tokenList  \\\n",
       "0  [迈向, 充满, 希望, 的, 新, 世纪, ——, 一九九八年, 新年, 讲话, （, 附...   \n",
       "\n",
       "                                            tokenIdx  \\\n",
       "0  [2189, 2033, 1116, 1566, 88, 379, 1461, 989, 2...   \n",
       "\n",
       "                                              redIdx  \\\n",
       "0  [171, 90, 88, 58, 80, 94, 133, 165, 124, 107, ...   \n",
       "\n",
       "                                          subcharIdx  \\\n",
       "0  [[521, 1143], [1160, 254, 698], [1007, 986], [...   \n",
       "\n",
       "                                                bmes  \n",
       "0  [0, 2, 0, 2, 0, 2, 3, 3, 0, 2, 0, 2, 0, 1, 1, ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_sequence(data_with_features):\n",
    "#     assert (np.max(list(map(len, data_with_features[\"tokenIdx\"])))) == (np.max(list(map(len, data_with_features[\"bmes\"]))))\n",
    "    return (np.max(list(map(len, data_with_features[\"tokenIdx\"]))))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 139 ms, sys: 236 ms, total: 375 ms\n",
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = extract_features(test, isTest=True)\n",
    "test_feature = temp\n",
    "# test_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>raws</th>\n",
       "      <th>tokenIdx</th>\n",
       "      <th>redIdx</th>\n",
       "      <th>subcharIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n",
       "      <td>共同创造美好的新世纪——二○○一年新年贺词</td>\n",
       "      <td>[2873, 1830, 3700, 1276, 3979, 927, 1461, 989,...</td>\n",
       "      <td>[218, 90, 71, 171, 23, 191, 133, 165, 124, 107...</td>\n",
       "      <td>[[400, 572], [254, 1054, 698], [614, 691], [52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>（二○○○年十二月三十一日）（附图片1张）</td>\n",
       "      <td>（二○○○年十二月三十一日）（附图片1张）</td>\n",
       "      <td>[1214, 859, 2907, 2907, 2907, 445, 2137, 859, ...</td>\n",
       "      <td>[203, 211, 203, 203, 203, 163, 51, 211, 94, 12...</td>\n",
       "      <td>[[1152], [1054, 1054], [1152], [1152], [1152],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>女士们，先生们，同志们，朋友们：</td>\n",
       "      <td>女士们，先生们，同志们，朋友们：</td>\n",
       "      <td>[4351, 4651, 1052, 1928, 3266, 3591, 1052, 192...</td>\n",
       "      <td>[191, 196, 15, 203, 88, 43, 15, 203, 90, 168, ...</td>\n",
       "      <td>[[813, 1054, 830], [962, 1054], [573, 51], [11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...</td>\n",
       "      <td>2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...</td>\n",
       "      <td>[3011, 2419, 2419, 3269, 445, 989, 445, 672, 2...</td>\n",
       "      <td>[203, 203, 203, 203, 163, 165, 163, 41, 196, 9...</td>\n",
       "      <td>[[1152], [1152], [1152], [1152], [830, 1102, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...</td>\n",
       "      <td>在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...</td>\n",
       "      <td>[2036, 2305, 891, 1538, 361, 572, 501, 1461, 3...</td>\n",
       "      <td>[194, 171, 91, 58, 126, 220, 168, 133, 81, 71,...</td>\n",
       "      <td>[[1054, 573, 538], [521, 179], [573, 697], [10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                              共同创造美好的新世纪——二○○一年新年贺词   \n",
       "1                              （二○○○年十二月三十一日）（附图片1张）   \n",
       "2                                   女士们，先生们，同志们，朋友们：   \n",
       "3  2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...   \n",
       "4  在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...   \n",
       "\n",
       "                                                raws  \\\n",
       "0                              共同创造美好的新世纪——二○○一年新年贺词   \n",
       "1                              （二○○○年十二月三十一日）（附图片1张）   \n",
       "2                                   女士们，先生们，同志们，朋友们：   \n",
       "3  2001年新年钟声即将敲响。人类社会前进的航船就要驶入21世纪的新航程。中国人民进入了向现代...   \n",
       "4  在这个激动人心的时刻，我很高兴通过中国国际广播电台、中央人民广播电台和中央电视台，向全国各族...   \n",
       "\n",
       "                                            tokenIdx  \\\n",
       "0  [2873, 1830, 3700, 1276, 3979, 927, 1461, 989,...   \n",
       "1  [1214, 859, 2907, 2907, 2907, 445, 2137, 859, ...   \n",
       "2  [4351, 4651, 1052, 1928, 3266, 3591, 1052, 192...   \n",
       "3  [3011, 2419, 2419, 3269, 445, 989, 445, 672, 2...   \n",
       "4  [2036, 2305, 891, 1538, 361, 572, 501, 1461, 3...   \n",
       "\n",
       "                                              redIdx  \\\n",
       "0  [218, 90, 71, 171, 23, 191, 133, 165, 124, 107...   \n",
       "1  [203, 211, 203, 203, 203, 163, 51, 211, 94, 12...   \n",
       "2  [191, 196, 15, 203, 88, 43, 15, 203, 90, 168, ...   \n",
       "3  [203, 203, 203, 203, 163, 165, 163, 41, 196, 9...   \n",
       "4  [194, 171, 91, 58, 126, 220, 168, 133, 81, 71,...   \n",
       "\n",
       "                                          subcharIdx  \n",
       "0  [[400, 572], [254, 1054, 698], [614, 691], [52...  \n",
       "1  [[1152], [1054, 1054], [1152], [1152], [1152],...  \n",
       "2  [[813, 1054, 830], [962, 1054], [573, 51], [11...  \n",
       "3  [[1152], [1152], [1152], [1152], [830, 1102, 1...  \n",
       "4  [[1054, 573, 538], [521, 179], [573, 697], [10...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n",
      "626\n",
      "1019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_longest = find_longest_sequence(train_feature)\n",
    "print(train_longest)\n",
    "\n",
    "test_longest = find_longest_sequence(test_feature)\n",
    "print(test_longest)\n",
    "\n",
    "seq_longest = np.max([train_longest,test_longest])\n",
    "print(seq_longest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out max sub char length \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subchar_seq_length = np.max(list(map(lambda x : len(getSubChar(x)),train_dic_char)))\n",
    "subchar_seq_length = 3\n",
    "subsubchar_padtok = oov_subchar + 1\n",
    "seq_length = seq_longest\n",
    "\n",
    "def padd_char(seq):\n",
    "    temp_char_seqs_padded = []\n",
    "    for item in seq[\"subcharIdx\"]:\n",
    "        temp_pad = pad_sequences(item, maxlen=subchar_seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=subsubchar_padtok)\n",
    "  \n",
    "        a = temp_pad\n",
    "        b = [[subsubchar_padtok for i in range(subchar_seq_length)] for _ in range(0, seq_length - len(temp_pad))]\n",
    "        if len(b) == 0:\n",
    "            c = a\n",
    "        else:\n",
    "            c = np.concatenate((a, b))\n",
    "        \n",
    "    # print(len(c))\n",
    "        temp_char_seqs_padded.append(c)\n",
    "  # print(len(temp_char_seqs_padded))\n",
    "    return temp_char_seqs_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subchar_seqs_padded = padd_char(train_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padding token index is 4699\n",
      "Example of padded token sequence:\n",
      "[3221 2873 3221 ... 4699 4699 4699]\n"
     ]
    }
   ],
   "source": [
    "seq_length = seq_longest\n",
    "\n",
    "# a new dummy token index, one more than OOV\n",
    "padtok = oov+1\n",
    "red_padtok = oov_redical + 1\n",
    "print('The padding token index is %i' % padtok)\n",
    "\n",
    "# use pad_sequences, padding or truncating at the end of the sequence (default is 'pre')\n",
    "train_seqs_padded = pad_sequences(train_feature['tokenIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
    "print('Example of padded token sequence:')\n",
    "print(train_seqs_padded[1])\n",
    "\n",
    "\n",
    "train_red_padded = pad_sequences(train_feature['redIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=red_padtok)\n",
    "\n",
    "\n",
    "train_subchar_seqs_padded = padd_char(train_feature)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare Test set.\n",
    "test_seqs_padded = pad_sequences(test_feature['tokenIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
    "\n",
    "test_rad_padded = pad_sequences(test_feature['redIdx'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=red_padtok)\n",
    "\n",
    "test_subchar_seqs_padded = padd_char(test_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 400,  572, 1192],\n",
       "       [ 254, 1054,  698],\n",
       "       [ 614,  691, 1192],\n",
       "       ...,\n",
       "       [1192, 1192, 1192],\n",
       "       [1192, 1192, 1192],\n",
       "       [1192, 1192, 1192]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subchar_seqs_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# get lists of named entity labels, padded with a null label (=3)\n",
    "padlab = 4\n",
    "train_labs_padded = pad_sequences(train_feature['bmes'].tolist(), maxlen=seq_length,\n",
    "                                  dtype='int32', padding='post', truncating='post', value=padlab)\n",
    "\n",
    "# convert those labels to one-hot encoding\n",
    "n_labs = 5\n",
    "train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]\n",
    "\n",
    "# # follow the print outputs below to see how the labels are transformed\n",
    "# print('Length of input sequence: %i' % len(train_labs_padded[1]))\n",
    "# print('Length of label sequence: %i' % len(train_labs_onehot[1]))\n",
    "# print(train_labs_padded[1][:11])\n",
    "# print(train_labs_onehot[1][:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "**Defining a neural network**\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input3 (InputLayer)        [(None, 1019, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tok_input1 (InputLayer)         [(None, 1019)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "red_input2 (InputLayer)         [(None, 1019)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 1019, 3, 128) 152704      char_input3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1019, 128)    601600      tok_input1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1019, 128)    30336       red_input2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 1019, 100)    71600       time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1019, 356)    0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1019, 100)    162800      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1019, 100)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 1019, 5)      505         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,019,545\n",
      "Trainable params: 1,019,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load Keras and TensorFlow\n",
    "\n",
    "\n",
    "\n",
    "# our final vocab size is the padding token + 1 (OR length of vocab + OOV + PAD)\n",
    "vocab_size = padtok+1\n",
    "red_size = red_padtok+1\n",
    "subchar_size = subsubchar_padtok + 1\n",
    "\n",
    "print(vocab_size==len(train_dic_char)+2)\n",
    "embed_size = 128 # y an embedding size of 128 (could tune this)\n",
    "\n",
    "# list of metrics to use: true & false positives, negatives, accuracy, precision, recall, area under the curve\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "# our model has the option for an label prediction bias, it's sequential, starts with an embedding layer, then bi-LSTM,\n",
    "# a dropout layer follows for regularisation, and a dense final layer with softmax activation to output class probabilities\n",
    "# we compile with the Adam optimizer at a low learning rate, use categorical cross-entropy as our loss function\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:        \n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    tok_input1 = keras.layers.Input(shape=(seq_length,), dtype='int32', name='tok_input1')\n",
    "    red_input2 = keras.layers.Input(shape=(seq_length,), dtype='int32', name='red_input2')\n",
    "    subchar_input3 = keras.layers.Input(shape=(seq_length,subchar_seq_length), dtype='int32', name='char_input3')\n",
    "\n",
    "    emb_char = keras.layers.TimeDistributed(keras.layers.Embedding(output_dim=embed_size, input_dim=subchar_size, input_length=3,  mask_zero=True, trainable=True))(subchar_input3)\n",
    "    char_enc = keras.layers.TimeDistributed(keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=False, dropout=0.2,recurrent_dropout=0.2)))(emb_char)\n",
    "        \n",
    "\n",
    "    x1 = keras.layers.Embedding(output_dim=embed_size, input_dim=vocab_size,  input_length=seq_length,  mask_zero=True, trainable=True)(tok_input1)\n",
    "    x2 = keras.layers.Embedding(output_dim=embed_size, input_dim=red_size,  input_length=seq_length, mask_zero=True, trainable=True)(red_input2)\n",
    "    x_cancat = keras.layers.concatenate([x1, x2, char_enc])\n",
    "#     x_cancat = (x1)\n",
    "    x_lstm = keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2))(x_cancat)\n",
    "    x_drop = keras.layers.Dropout(0.5)(x_lstm)\n",
    "    main_output = keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias))(x_drop)\n",
    "    model = keras.models.Model(inputs=[tok_input1,red_input2, subchar_input3], outputs= main_output)\n",
    "#     model = keras.models.Model(inputs=[tok_input1, pos_input2, char_input3], outputs= main_output)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Old lstm model\n",
    "#     model = keras.Sequential([\n",
    "#         keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True),\n",
    "#         keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2)),  # 2 directions, 50 units each, concatenated (can change this)\n",
    "#         keras.layers.Dropout(0.5),\n",
    "#         keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias)),\n",
    "#     ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# early stopping criteria based on area under the curve: will stop if no improvement after 10 epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n",
    "\n",
    "# the number of training epochs we'll use, and the batch size (how many texts are input at once)\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "print('**Defining a neural network**')\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # evaluate our initial model\n",
    "# results = model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=0)\n",
    "# print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 17589578, 0: 585226, 2: 585226, 3: 524721, 1: 131275})\n",
      "19416026\n",
      "Initial bias:\n",
      "[0.03014138938627297, 0.006761167295511451, 0.03014138938627297, 0.027025149224666263, 0.9059309047072763]\n"
     ]
    }
   ],
   "source": [
    "# figure out the label distribution in our fixed-length texts\n",
    "from collections import Counter\n",
    "all_labs = [l for lab in train_labs_padded for l in lab]\n",
    "label_count = Counter(all_labs)\n",
    "total_labs = len(all_labs)\n",
    "print(label_count)\n",
    "print(total_labs)\n",
    "\n",
    "# use this to define an initial model bias\n",
    "initial_bias=[(label_count[0]/total_labs), (label_count[1]/total_labs),\n",
    "              (label_count[2]/total_labs), (label_count[3]/total_labs), (label_count[4]/total_labs)]\n",
    "print('Initial bias:')\n",
    "print(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence dimensions (n.docs, seq.length):\n",
      "(19054, 1019)\n",
      "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
      "(19054, 1019, 5)\n"
     ]
    }
   ],
   "source": [
    "# prepare sequences and labels as numpy arrays, check dimensions\n",
    "X = np.array(train_seqs_padded)\n",
    "X_red = np.array(train_red_padded)\n",
    "X_sub = np.array(train_subchar_seqs_padded)\n",
    "y = np.array(train_labs_onehot)\n",
    "print('Input sequence dimensions (n.docs, seq.length):')\n",
    "print(X.shape)\n",
    "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "13337/13337 [==============================] - 994s 75ms/step - loss: 0.0376 - tp: 13368166.0000 - fp: 134333.0000 - tn: 54227164.0000 - fn: 222237.0000 - accuracy: 0.9948 - precision: 0.9901 - recall: 0.9836 - auc: 0.9997 - val_loss: 0.0258 - val_tp: 5766751.0000 - val_fp: 44656.0000 - val_tn: 23257868.0000 - val_fn: 58872.0000 - val_accuracy: 0.9964 - val_precision: 0.9923 - val_recall: 0.9899 - val_auc: 0.9998\n",
      "Epoch 2/10\n",
      "13337/13337 [==============================] - ETA: 0s - loss: 0.0246 - tp: 13460860.0000 - fp: 97751.0000 - tn: 54263776.0000 - fn: 129543.0000 - accuracy: 0.9967 - precision: 0.9928 - recall: 0.9905 - auc: 0.9998"
     ]
    }
   ],
   "source": [
    "# re-initiate model with bias\n",
    "model = make_model(output_bias=initial_bias)\n",
    "# and fit...\n",
    "model.fit([X,X_red,X_sub], y, batch_size=1, epochs=10, callbacks = [early_stopping],  validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_seqs_padded)\n",
    "X_test_rad = np.array(test_rad_padded)\n",
    "# preds = np.argmax(model.predict(X_test), axis=-1)\n",
    "preds = np.argmax(model.predict([X_test,X_test_rad]), axis=-1)\n",
    "flat_preds = [p for pred in preds for p in pred]\n",
    "print(Counter(flat_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def postEditPred(pred):\n",
    "    print(pred)\n",
    "    ans = []\n",
    "    for item in pred:\n",
    "        if item == 0 or item == 3:\n",
    "            ans.append(1)\n",
    "        else:\n",
    "            ans.append(0)\n",
    "            \n",
    "    assert len(ans) == len(pred)\n",
    "    return ans\n",
    "\n",
    "def splitSentence(st, pred):\n",
    "    temp_st = st\n",
    "    temp_pred = pred\n",
    "    temp_pred[0] = 2\n",
    "    buf = []\n",
    "    result = []\n",
    "    for (pre,char) in zip(temp_pred,temp_st):\n",
    "        if(pre == 0 or pre == 3):\n",
    "            result.append(buf)\n",
    "            buf = []\n",
    "        buf.append(char)\n",
    "    if(len(buf) > 0):\n",
    "        result.append(buf)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def sentencePrediction(dataset,prediction):\n",
    "    data_temp = dataset.copy()\n",
    "    assert len(data_temp) == len(prediction)\n",
    "    raw_sents = data_temp[\"raws\"]\n",
    "    assert len(raw_sents) == len(prediction)\n",
    "    ans = []\n",
    "    for (st,pred) in zip(raw_sents,prediction):\n",
    "        sptSt = splitSentence(st,pred)\n",
    "        ans.append(sptSt)\n",
    "    assert len(ans) == len(data_temp)\n",
    "    data_temp[\"tokenList\"]= ans\n",
    "    return data_temp\n",
    "    \n",
    "def convertToPredSts(dataset):\n",
    "    data_temp = dataset.copy()\n",
    "    tokenList = data_temp[\"tokenList\"]\n",
    "    ans = []\n",
    "    \n",
    "    for item in tokenList:\n",
    "        temp_st = list(map(lambda x : \"\".join(x), item))\n",
    "        temp = \"  \".join(temp_st)\n",
    "        temp = temp + \"  \"\n",
    "        ans.append(temp)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pred = sentencePrediction(test_feature,preds)\n",
    "output_sts = convertToPredSts(result_pred)\n",
    "\n",
    "f = open(\"./ans.txt\", \"w\")\n",
    "f.write(\"\\n\".join(output_sts))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
